# Phase 5: éŸ³å£°ãƒ»æ„Ÿæƒ…ã‚·ã‚¹ãƒ†ãƒ  ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ

æœŸé–“: 4é€±é–“ï¼ˆDay 99-126ï¼‰

## Week 15-16: éŸ³å£°åˆæˆ

### Day 99-100: VOICEVOXçµ±åˆ
- [ ] VOICEVOX API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…
  ```typescript
  class VoiceVoxClient {
    private baseUrl = process.env.VOICEVOX_URL || 'http://localhost:50021';
    
    async getSpeakers(): Promise<Speaker[]> {
      const response = await fetch(`${this.baseUrl}/speakers`);
      return response.json();
    }
    
    async synthesize(text: string, options: SynthesisOptions): Promise<ArrayBuffer> {
      // éŸ³å£°ã‚¯ã‚¨ãƒªç”Ÿæˆ
      const queryResponse = await fetch(
        `${this.baseUrl}/audio_query?text=${encodeURIComponent(text)}&speaker=${options.speaker}`,
        { method: 'POST' }
      );
      const query = await queryResponse.json();
      
      // ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
      if (options.speed) query.speedScale = options.speed;
      if (options.pitch) query.pitchScale = options.pitch;
      if (options.intonation) query.intonationScale = options.intonation;
      
      // éŸ³å£°åˆæˆ
      const audioResponse = await fetch(
        `${this.baseUrl}/synthesis?speaker=${options.speaker}`,
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(query)
        }
      );
      
      return audioResponse.arrayBuffer();
    }
  }
  ```
- [ ] è©±è€…é¸æŠã‚·ã‚¹ãƒ†ãƒ 
  ```typescript
  interface SpeakerProfile {
    id: number;
    name: string;
    styles: StyleInfo[];
    sampleAudio?: string;
    personality?: string;
  }
  
  class SpeakerSelector {
    private speakers: SpeakerProfile[] = [];
    
    async loadSpeakers() {
      const voicevoxSpeakers = await this.voicevoxClient.getSpeakers();
      this.speakers = voicevoxSpeakers.map(this.mapToProfile);
    }
    
    selectForMode(mode: 'ASD' | 'NT', emotion?: EmotionType): number {
      // ãƒ¢ãƒ¼ãƒ‰ã¨æ„Ÿæƒ…ã«é©ã—ãŸè©±è€…ã‚’é¸æŠ
      if (mode === 'ASD') {
        // ã‚ˆã‚Šè½ã¡ç€ã„ãŸã€ä¸€å®šã®ãƒˆãƒ¼ãƒ³ã®è©±è€…
        return this.speakers.find(s => 
          s.personality === 'calm' || s.id === 3
        )?.id || 3;
      } else {
        // ã‚ˆã‚Šè¡¨ç¾è±Šã‹ãªè©±è€…
        return this.speakers.find(s => 
          s.personality === 'expressive' || s.id === 1
        )?.id || 1;
      }
    }
    
    getStyleForEmotion(speakerId: number, emotion: EmotionType): number {
      const speaker = this.speakers.find(s => s.id === speakerId);
      const emotionStyleMap = {
        joy: 'happy',
        sadness: 'sad',
        anger: 'angry',
        neutral: 'normal'
      };
      
      return speaker?.styles.find(s => 
        s.name === emotionStyleMap[emotion]
      )?.id || 0;
    }
  }
  ```
- [ ] ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
  ```typescript
  class VoiceParameterAdjuster {
    adjustForMode(baseParams: VoiceParameters, mode: 'ASD' | 'NT'): VoiceParameters {
      if (mode === 'ASD') {
        return {
          ...baseParams,
          speedScale: 0.95,      // ã‚„ã‚„é…ã‚
          pitchScale: 1.0,       // ä¸€å®šã®ãƒ”ãƒƒãƒ
          intonationScale: 0.7,  // æŠ‘æšã‚’æŠ‘ãˆã‚‹
          volumeScale: 0.9,      // ã‚„ã‚„æ§ãˆã‚
          prePhonemeLength: 0.1, // ç™ºè©±å‰ã®é–“
          postPhonemeLength: 0.2 // ç™ºè©±å¾Œã®é–“
        };
      } else {
        return {
          ...baseParams,
          speedScale: 1.05,      // è‡ªç„¶ãªé€Ÿåº¦
          pitchScale: 1.0,       // æ¨™æº–ãƒ”ãƒƒãƒ
          intonationScale: 1.2,  // è±Šã‹ãªæŠ‘æš
          volumeScale: 1.0,      // æ¨™æº–éŸ³é‡
          prePhonemeLength: 0.05,
          postPhonemeLength: 0.1
        };
      }
    }
    
    adjustForEmotion(params: VoiceParameters, emotion: EmotionType, intensity: number): VoiceParameters {
      const emotionAdjustments = {
        joy: {
          speedScale: 1.1 * intensity,
          pitchScale: 1.1 * intensity,
          intonationScale: 1.3 * intensity
        },
        sadness: {
          speedScale: 0.9 * (1 - intensity * 0.1),
          pitchScale: 0.9 * (1 - intensity * 0.1),
          intonationScale: 0.8 * (1 - intensity * 0.2)
        },
        anger: {
          speedScale: 1.2 * intensity,
          pitchScale: 0.95,
          volumeScale: 1.2 * intensity
        },
        fear: {
          speedScale: 1.15 * intensity,
          pitchScale: 1.15 * intensity,
          intonationScale: 1.4 * intensity
        }
      };
      
      const adjustment = emotionAdjustments[emotion] || {};
      return { ...params, ...adjustment };
    }
  }
  ```

### Day 101-102: éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
- [ ] ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè£…
  ```typescript
  class AudioStreamManager {
    private audioContext: AudioContext;
    private sourceNodes: Map<string, AudioBufferSourceNode> = new Map();
    private streamBuffer: ArrayBuffer[] = [];
    
    constructor() {
      this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
    }
    
    async streamAudio(audioStream: ReadableStream<Uint8Array>) {
      const reader = audioStream.getReader();
      const chunks: Uint8Array[] = [];
      
      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          
          chunks.push(value);
          
          // ä¸€å®šé‡æºœã¾ã£ãŸã‚‰å†ç”Ÿé–‹å§‹
          if (chunks.length >= 3) {
            await this.playChunks(chunks.splice(0, chunks.length));
          }
        }
        
        // æ®‹ã‚Šã‚’å†ç”Ÿ
        if (chunks.length > 0) {
          await this.playChunks(chunks);
        }
      } finally {
        reader.releaseLock();
      }
    }
    
    private async playChunks(chunks: Uint8Array[]) {
      const combinedArray = this.combineChunks(chunks);
      const audioBuffer = await this.audioContext.decodeAudioData(combinedArray.buffer);
      
      const source = this.audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(this.audioContext.destination);
      
      // å‰ã®éŸ³å£°ã¨ã®åŒæœŸ
      const currentTime = this.audioContext.currentTime;
      const startTime = this.getNextStartTime(currentTime);
      source.start(startTime);
      
      this.sourceNodes.set(generateId(), source);
    }
    
    private combineChunks(chunks: Uint8Array[]): Uint8Array {
      const totalLength = chunks.reduce((sum, chunk) => sum + chunk.length, 0);
      const combined = new Uint8Array(totalLength);
      let offset = 0;
      
      for (const chunk of chunks) {
        combined.set(chunk, offset);
        offset += chunk.length;
      }
      
      return combined;
    }
  }
  ```
- [ ] ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°åˆ¶å¾¡
  ```typescript
  class AudioBufferController {
    private bufferSize = 3; // ç§’
    private bufferQueue: AudioData[] = [];
    private isPlaying = false;
    private onBufferLow?: () => void;
    
    async addToBuffer(audioData: AudioData) {
      this.bufferQueue.push(audioData);
      
      if (!this.isPlaying && this.getBufferDuration() >= this.bufferSize) {
        this.startPlayback();
      }
    }
    
    private getBufferDuration(): number {
      return this.bufferQueue.reduce((sum, data) => sum + data.duration, 0);
    }
    
    private async startPlayback() {
      this.isPlaying = true;
      
      while (this.bufferQueue.length > 0) {
        const audioData = this.bufferQueue.shift()!;
        await this.playAudio(audioData);
        
        // ãƒãƒƒãƒ•ã‚¡ãŒå°‘ãªããªã£ãŸã‚‰é€šçŸ¥
        if (this.getBufferDuration() < this.bufferSize / 2 && this.onBufferLow) {
          this.onBufferLow();
        }
      }
      
      this.isPlaying = false;
    }
    
    private async playAudio(audioData: AudioData): Promise<void> {
      return new Promise(resolve => {
        const source = this.audioContext.createBufferSource();
        source.buffer = audioData.buffer;
        source.connect(this.audioContext.destination);
        source.onended = () => resolve();
        source.start();
      });
    }
  }
  ```
- [ ] åŒæœŸåˆ¶å¾¡
  ```typescript
  class AudioSyncController {
    private messageQueue: SyncedMessage[] = [];
    private currentMessageId: string | null = null;
    
    async queueMessage(message: SyncedMessage) {
      this.messageQueue.push(message);
      
      if (!this.currentMessageId) {
        await this.processNextMessage();
      }
    }
    
    private async processNextMessage() {
      if (this.messageQueue.length === 0) {
        this.currentMessageId = null;
        return;
      }
      
      const message = this.messageQueue.shift()!;
      this.currentMessageId = message.id;
      
      // ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º
      this.displayText(message.text);
      
      // éŸ³å£°å†ç”Ÿï¼ˆãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã¨åŒæœŸï¼‰
      await Promise.all([
        this.playAudio(message.audio),
        this.animateLipSync(message.lipSyncData)
      ]);
      
      // æ¬¡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†
      await this.processNextMessage();
    }
    
    private async animateLipSync(lipSyncData: LipSyncData) {
      for (const frame of lipSyncData.frames) {
        await this.waitForTimestamp(frame.timestamp);
        this.updateMouthShape(frame.vowel);
      }
    }
    
    private updateMouthShape(vowel: Vowel) {
      const mouthShapes = {
        'a': { A: 1.0, I: 0, U: 0, E: 0, O: 0 },
        'i': { A: 0, I: 1.0, U: 0, E: 0, O: 0 },
        'u': { A: 0, I: 0, U: 1.0, E: 0, O: 0 },
        'e': { A: 0, I: 0, U: 0, E: 1.0, O: 0 },
        'o': { A: 0, I: 0, U: 0, E: 0, O: 1.0 },
        'n': { A: 0.2, I: 0, U: 0, E: 0, O: 0 }
      };
      
      const shape = mouthShapes[vowel] || mouthShapes['n'];
      this.vrmManager.setMouthShape(shape);
    }
  }
  ```

### Day 103-104: ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯å®Ÿè£…
- [ ] éŸ³ç´ è§£æ
  ```typescript
  class PhonemeAnalyzer {
    private audioContext: AudioContext;
    
    async analyzePhonemes(audioBuffer: AudioBuffer): Promise<PhonemeData[]> {
      const audioData = audioBuffer.getChannelData(0);
      const sampleRate = audioBuffer.sampleRate;
      
      // FFTã«ã‚ˆã‚‹å‘¨æ³¢æ•°è§£æ
      const fftSize = 2048;
      const hopSize = fftSize / 4;
      const phonemes: PhonemeData[] = [];
      
      for (let i = 0; i < audioData.length - fftSize; i += hopSize) {
        const segment = audioData.slice(i, i + fftSize);
        const spectrum = this.fft(segment);
        const formants = this.extractFormants(spectrum, sampleRate);
        const phoneme = this.classifyPhoneme(formants);
        
        phonemes.push({
          timestamp: i / sampleRate,
          phoneme: phoneme.type,
          confidence: phoneme.confidence,
          vowel: this.phonemeToVowel(phoneme.type)
        });
      }
      
      return this.smoothPhonemes(phonemes);
    }
    
    private extractFormants(spectrum: Float32Array, sampleRate: number): Formants {
      // ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå‘¨æ³¢æ•°ã®æŠ½å‡º
      const peaks = this.findSpectralPeaks(spectrum);
      
      return {
        f1: peaks[0]?.frequency || 0,
        f2: peaks[1]?.frequency || 0,
        f3: peaks[2]?.frequency || 0
      };
    }
    
    private classifyPhoneme(formants: Formants): ClassifiedPhoneme {
      // ãƒ•ã‚©ãƒ«ãƒãƒ³ãƒˆå‘¨æ³¢æ•°ã«åŸºã¥ãéŸ³ç´ åˆ†é¡
      const vowelFormants = {
        'a': { f1: [700, 850], f2: [1100, 1300] },
        'i': { f1: [250, 350], f2: [2200, 2500] },
        'u': { f1: [300, 400], f2: [700, 900] },
        'e': { f1: [400, 600], f2: [1900, 2200] },
        'o': { f1: [450, 650], f2: [850, 1100] }
      };
      
      let bestMatch = { type: 'n', confidence: 0 };
      
      for (const [vowel, ranges] of Object.entries(vowelFormants)) {
        const f1Match = this.isInRange(formants.f1, ranges.f1);
        const f2Match = this.isInRange(formants.f2, ranges.f2);
        const confidence = (f1Match + f2Match) / 2;
        
        if (confidence > bestMatch.confidence) {
          bestMatch = { type: vowel, confidence };
        }
      }
      
      return bestMatch;
    }
  }
  ```
- [ ] å£å½¢çŠ¶ãƒãƒƒãƒ”ãƒ³ã‚°
  ```typescript
  class LipSyncMapper {
    private blendShapeMap = {
      'a': { mouthOpen: 0.8, mouthWide: 0.3 },
      'i': { mouthOpen: 0.2, mouthWide: 0.7, mouthPucker: 0.1 },
      'u': { mouthOpen: 0.3, mouthPucker: 0.8 },
      'e': { mouthOpen: 0.4, mouthWide: 0.5 },
      'o': { mouthOpen: 0.5, mouthPucker: 0.5 },
      'n': { mouthOpen: 0.1, neutral: 0.9 }
    };
    
    generateLipSyncData(
      text: string,
      phonemes: PhonemeData[],
      duration: number
    ): LipSyncAnimation {
      const morphTargets: MorphTargetFrame[] = [];
      
      // ãƒ†ã‚­ã‚¹ãƒˆã¨éŸ³ç´ ã‚’å¯¾å¿œä»˜ã‘
      const alignedPhonemes = this.alignPhonemesToText(text, phonemes);
      
      for (const phoneme of alignedPhonemes) {
        const blendShapes = this.blendShapeMap[phoneme.vowel] || this.blendShapeMap['n'];
        
        morphTargets.push({
          timestamp: phoneme.timestamp,
          blendShapes: this.smoothBlendShapes(blendShapes, phoneme.confidence),
          duration: phoneme.duration
        });
      }
      
      // è£œé–“ãƒ•ãƒ¬ãƒ¼ãƒ ã®ç”Ÿæˆ
      return {
        frames: this.interpolateFrames(morphTargets, 60), // 60fps
        duration: duration
      };
    }
    
    private interpolateFrames(
      keyframes: MorphTargetFrame[],
      fps: number
    ): LipSyncFrame[] {
      const frames: LipSyncFrame[] = [];
      const frameTime = 1000 / fps;
      
      for (let i = 0; i < keyframes.length - 1; i++) {
        const current = keyframes[i];
        const next = keyframes[i + 1];
        const framesToGenerate = Math.floor(
          (next.timestamp - current.timestamp) * 1000 / frameTime
        );
        
        for (let j = 0; j < framesToGenerate; j++) {
          const t = j / framesToGenerate;
          frames.push({
            timestamp: current.timestamp + (t * (next.timestamp - current.timestamp)),
            blendShapes: this.lerpBlendShapes(current.blendShapes, next.blendShapes, t)
          });
        }
      }
      
      return frames;
    }
  }
  ```
- [ ] ã‚¿ã‚¤ãƒŸãƒ³ã‚°èª¿æ•´
  ```typescript
  class LipSyncTimingAdjuster {
    private offsetMs = -50; // éŸ³å£°ã‚ˆã‚Šå°‘ã—æ—©ã‚ã«å£ã‚’å‹•ã‹ã™
    
    adjustTiming(lipSyncData: LipSyncAnimation, mode: 'ASD' | 'NT'): LipSyncAnimation {
      // ãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦ã‚¿ã‚¤ãƒŸãƒ³ã‚°èª¿æ•´
      const offset = mode === 'ASD' 
        ? this.offsetMs - 20  // ASDãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚ˆã‚Šæ…é‡ãªå£ã®å‹•ã
        : this.offsetMs;
      
      const adjustedFrames = lipSyncData.frames.map(frame => ({
        ...frame,
        timestamp: Math.max(0, frame.timestamp + offset / 1000)
      }));
      
      // ASDãƒ¢ãƒ¼ãƒ‰ã§ã¯å£ã®å‹•ãã‚’æ§ãˆã‚ã«
      if (mode === 'ASD') {
        adjustedFrames.forEach(frame => {
          Object.keys(frame.blendShapes).forEach(key => {
            frame.blendShapes[key] *= 0.7; // 70%ã«æŠ‘åˆ¶
          });
        });
      }
      
      return {
        ...lipSyncData,
        frames: adjustedFrames
      };
    }
    
    syncWithAudio(
      lipSyncData: LipSyncAnimation,
      audioStartTime: number
    ): AnimationController {
      return new AnimationController({
        startTime: audioStartTime,
        animation: lipSyncData,
        onFrame: (frame) => {
          this.vrmManager.setBlendShapes(frame.blendShapes);
        },
        onComplete: () => {
          this.vrmManager.resetMouth();
        }
      });
    }
  }
  ```

### Day 105-106: éŸ³å£°å…¥åŠ›æº–å‚™
- [ ] Web Speech APIçµ±åˆ
  ```typescript
  class SpeechRecognitionManager {
    private recognition: SpeechRecognition;
    private isListening = false;
    
    constructor() {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      this.recognition = new SpeechRecognition();
      
      this.setupRecognition();
    }
    
    private setupRecognition() {
      this.recognition.lang = 'ja-JP';
      this.recognition.continuous = true;
      this.recognition.interimResults = true;
      this.recognition.maxAlternatives = 3;
      
      this.recognition.onresult = this.handleResult.bind(this);
      this.recognition.onerror = this.handleError.bind(this);
      this.recognition.onend = this.handleEnd.bind(this);
    }
    
    start() {
      if (!this.isListening) {
        this.recognition.start();
        this.isListening = true;
      }
    }
    
    stop() {
      if (this.isListening) {
        this.recognition.stop();
        this.isListening = false;
      }
    }
    
    private handleResult(event: SpeechRecognitionEvent) {
      const results = event.results;
      const currentResult = results[results.length - 1];
      
      if (currentResult.isFinal) {
        const transcript = currentResult[0].transcript;
        const confidence = currentResult[0].confidence;
        
        this.onTranscript?.({
          text: transcript,
          confidence: confidence,
          alternatives: Array.from(currentResult).map(alt => ({
            text: alt.transcript,
            confidence: alt.confidence
          }))
        });
      } else {
        // ä¸­é–“çµæœ
        this.onInterimTranscript?.({
          text: currentResult[0].transcript,
          confidence: currentResult[0].confidence
        });
      }
    }
    
    private handleError(event: SpeechRecognitionErrorEvent) {
      console.error('Speech recognition error:', event.error);
      
      const errorHandlers = {
        'no-speech': () => this.onNoSpeech?.(),
        'audio-capture': () => this.onAudioCaptureError?.(),
        'not-allowed': () => this.onPermissionDenied?.(),
        'network': () => this.onNetworkError?.()
      };
      
      errorHandlers[event.error]?.();
    }
  }
  ```
- [ ] éŸ³å£°èªè­˜è¨­å®š
  ```typescript
  interface VoiceInputSettings {
    language: string;
    continuous: boolean;
    interimResults: boolean;
    maxAlternatives: number;
    silenceDetection: boolean;
    noiseReduction: boolean;
  }
  
  class VoiceInputConfigurator {
    private settings: VoiceInputSettings = {
      language: 'ja-JP',
      continuous: true,
      interimResults: true,
      maxAlternatives: 3,
      silenceDetection: true,
      noiseReduction: true
    };
    
    configureForMode(mode: 'ASD' | 'NT'): VoiceInputSettings {
      if (mode === 'ASD') {
        // ASDãƒ¢ãƒ¼ãƒ‰ã§ã¯æ˜ç¢ºãªç™ºè©±ã‚’æœŸå¾…
        return {
          ...this.settings,
          silenceDetection: true,
          continuous: false, // ä¸€åº¦ã«ä¸€ã¤ã®æ–‡ã‚’å‡¦ç†
          interimResults: false // æœ€çµ‚çµæœã®ã¿
        };
      } else {
        // NTãƒ¢ãƒ¼ãƒ‰ã§ã¯è‡ªç„¶ãªä¼šè©±ãƒ•ãƒ­ãƒ¼
        return {
          ...this.settings,
          silenceDetection: true,
          continuous: true,
          interimResults: true
        };
      }
    }
    
    async requestMicrophonePermission(): Promise<boolean> {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        stream.getTracks().forEach(track => track.stop());
        return true;
      } catch (error) {
        console.error('Microphone permission denied:', error);
        return false;
      }
    }
  }
  ```
- [ ] ãƒã‚¤ã‚ºå‡¦ç†
  ```typescript
  class NoiseProcessor {
    private audioContext: AudioContext;
    private noiseGate: DynamicsCompressorNode;
    
    setupNoiseReduction(stream: MediaStream): MediaStream {
      this.audioContext = new AudioContext();
      const source = this.audioContext.createMediaStreamSource(stream);
      
      // ãƒã‚¤ã‚ºã‚²ãƒ¼ãƒˆ
      this.noiseGate = this.audioContext.createDynamicsCompressor();
      this.noiseGate.threshold.value = -50;
      this.noiseGate.knee.value = 40;
      this.noiseGate.ratio.value = 12;
      this.noiseGate.attack.value = 0.003;
      this.noiseGate.release.value = 0.25;
      
      // ãƒã‚¤ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆä½å‘¨æ³¢ãƒã‚¤ã‚ºé™¤å»ï¼‰
      const highpass = this.audioContext.createBiquadFilter();
      highpass.type = 'highpass';
      highpass.frequency.value = 100;
      
      // ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆé«˜å‘¨æ³¢ãƒã‚¤ã‚ºé™¤å»ï¼‰
      const lowpass = this.audioContext.createBiquadFilter();
      lowpass.type = 'lowpass';
      lowpass.frequency.value = 8000;
      
      // æ¥ç¶š
      source.connect(highpass);
      highpass.connect(lowpass);
      lowpass.connect(this.noiseGate);
      
      // å‡¦ç†æ¸ˆã¿ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’è¿”ã™
      const destination = this.audioContext.createMediaStreamDestination();
      this.noiseGate.connect(destination);
      
      return destination.stream;
    }
    
    detectSilence(analyser: AnalyserNode, threshold: number = -50): boolean {
      const bufferLength = analyser.frequencyBinCount;
      const dataArray = new Float32Array(bufferLength);
      analyser.getFloatFrequencyData(dataArray);
      
      const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;
      return average < threshold;
    }
  }
  ```

## Week 17-18: é«˜åº¦ãªæ„Ÿæƒ…è¡¨ç¾

### Day 107-108: å†…éƒ¨æ„Ÿæƒ…vså¤–éƒ¨è¡¨ç¾ã®å®Ÿè£…
- [ ] æ„Ÿæƒ…ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆ
  ```typescript
  interface EmotionModel {
    // åŸºæœ¬æ„Ÿæƒ…
    primary: {
      joy: number;
      sadness: number;
      anger: number;
      fear: number;
      surprise: number;
      disgust: number;
    };
    
    // è¤‡åˆæ„Ÿæƒ…
    secondary: {
      excitement: number;     // joy + surprise
      disappointment: number; // sadness + surprise
      anxiety: number;       // fear + surprise
      frustration: number;   // anger + sadness
    };
    
    // æ„Ÿæƒ…ã®æ¬¡å…ƒ
    dimensions: {
      valence: number;    // -1 (negative) to 1 (positive)
      arousal: number;    // 0 (calm) to 1 (excited)
      dominance: number;  // 0 (submissive) to 1 (dominant)
    };
    
    // æ™‚ç³»åˆ—å¤‰åŒ–
    history: EmotionSnapshot[];
    
    // æ„Ÿæƒ…ã®æ…£æ€§
    inertia: number; // 0 (å³åº§ã«å¤‰åŒ–) to 1 (ã‚†ã£ãã‚Šå¤‰åŒ–)
  }
  
  class EmotionEngine {
    private currentEmotion: EmotionModel;
    private mode: 'ASD' | 'NT';
    
    updateEmotion(
      stimulus: EmotionalStimulus,
      context: ConversationContext
    ): EmotionUpdate {
      // 1. åˆºæ¿€ã‹ã‚‰åŸºæœ¬æ„Ÿæƒ…ã‚’è¨ˆç®—
      const rawEmotion = this.calculateRawEmotion(stimulus);
      
      // 2. æ–‡è„ˆã«ã‚ˆã‚‹èª¿æ•´
      const contextualizedEmotion = this.applyContext(rawEmotion, context);
      
      // 3. æ„Ÿæƒ…ã®æ…£æ€§ã‚’é©ç”¨
      const smoothedEmotion = this.applyInertia(
        contextualizedEmotion,
        this.currentEmotion,
        this.currentEmotion.inertia
      );
      
      // 4. ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®å†…å¤–åˆ†é›¢
      const { internal, external } = this.separateInternalExternal(
        smoothedEmotion,
        this.mode
      );
      
      // 5. å±¥æ­´æ›´æ–°
      this.updateHistory(internal);
      
      return {
        internal,
        external,
        delta: this.calculateDelta(this.currentEmotion, internal)
      };
    }
    
    private separateInternalExternal(
      emotion: EmotionModel,
      mode: 'ASD' | 'NT'
    ): { internal: EmotionModel; external: EmotionModel } {
      if (mode === 'ASD') {
        // ASD: å†…éƒ¨ã¯100%ã€å¤–éƒ¨ã¯30%
        const external = this.scaleEmotion(emotion, 0.3);
        
        // å¤–éƒ¨è¡¨ç¾ã¯ã‚ˆã‚Šä¸­ç«‹çš„ã«
        external.dimensions.arousal *= 0.5;
        external.dimensions.dominance *= 0.7;
        
        return { internal: emotion, external };
      } else {
        // NT: å†…å¤–ãŒã‚ˆã‚Šä¸€è‡´ï¼ˆå¤–éƒ¨ã¯80%ï¼‰
        const external = this.scaleEmotion(emotion, 0.8);
        
        // ç¤¾ä¼šçš„èª¿æ•´ã‚’é©ç”¨
        external.primary = this.applySocialModulation(external.primary);
        
        return { internal: emotion, external };
      }
    }
  }
  ```
- [ ] å†…å¤–åˆ†é›¢ãƒ­ã‚¸ãƒƒã‚¯
  ```typescript
  class InternalExternalSeparator {
    separateEmotions(
      emotion: EmotionState,
      mode: 'ASD' | 'NT',
      socialContext?: SocialContext
    ): SeparatedEmotions {
      const separationRules = {
        ASD: {
          expressionRatio: 0.3,
          verbalizationThreshold: 0.7, // æ„Ÿæƒ…ã‚’è¨€èªåŒ–ã™ã‚‹é–¾å€¤
          maskingLevel: 0.1, // ç¤¾ä¼šçš„ãƒã‚¹ã‚­ãƒ³ã‚°ã¯æœ€å°
          processingDelay: 2000, // æ„Ÿæƒ…å‡¦ç†ã®é…å»¶
          regulationDifficulty: 0.8 // æ„Ÿæƒ…èª¿æ•´ã®å›°é›£ã•
        },
        NT: {
          expressionRatio: 0.8,
          verbalizationThreshold: 0.3,
          maskingLevel: 0.6, // é©åˆ‡ãªç¤¾ä¼šçš„ãƒã‚¹ã‚­ãƒ³ã‚°
          processingDelay: 500,
          regulationDifficulty: 0.3
        }
      };
      
      const rules = separationRules[mode];
      
      // å†…éƒ¨æ„Ÿæƒ…ã®å‡¦ç†
      const internalIntensity = emotion.intensity;
      const internalComplexity = this.calculateComplexity(emotion);
      
      // å¤–éƒ¨è¡¨ç¾ã®è¨ˆç®—
      let externalIntensity = internalIntensity * rules.expressionRatio;
      
      // ç¤¾ä¼šçš„æ–‡è„ˆã«ã‚ˆã‚‹èª¿æ•´
      if (socialContext) {
        externalIntensity = this.adjustForSocialContext(
          externalIntensity,
          socialContext,
          rules.maskingLevel
        );
      }
      
      // æ„Ÿæƒ…ã®è¨€èªåŒ–
      const shouldVerbalize = internalIntensity > rules.verbalizationThreshold;
      const verbalExpression = shouldVerbalize
        ? this.generateVerbalExpression(emotion, mode)
        : null;
      
      // èº«ä½“çš„è¡¨ç¾
      const physicalExpression = this.generatePhysicalExpression(
        emotion,
        externalIntensity,
        mode
      );
      
      return {
        internal: {
          emotion: emotion,
          intensity: internalIntensity,
          complexity: internalComplexity,
          processingTime: rules.processingDelay
        },
        external: {
          emotion: this.simplifyEmotion(emotion),
          intensity: externalIntensity,
          verbal: verbalExpression,
          physical: physicalExpression,
          congruence: this.calculateCongruence(internalIntensity, externalIntensity)
        },
        metadata: {
          mode: mode,
          timestamp: Date.now(),
          socialContext: socialContext
        }
      };
    }
    
    private generateVerbalExpression(
      emotion: EmotionState,
      mode: 'ASD' | 'NT'
    ): VerbalExpression {
      if (mode === 'ASD') {
        // ç›´æ¥çš„ã§æ˜ç¤ºçš„ãªæ„Ÿæƒ…è¡¨ç¾
        return {
          text: `ç§ã¯${this.getEmotionLabel(emotion)}ã‚’æ„Ÿã˜ã¦ã„ã¾ã™`,
          directness: 1.0,
          clarity: 1.0,
          metaphorical: 0.0
        };
      } else {
        // ã‚ˆã‚Šé–“æ¥çš„ã§æ–‡è„ˆä¾å­˜ã®è¡¨ç¾
        return {
          text: this.getNaturalEmotionExpression(emotion),
          directness: 0.4,
          clarity: 0.7,
          metaphorical: 0.6
        };
      }
    }
  }
  ```
- [ ] å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ 
  ```typescript
  class EmotionVisualizationSystem {
    private canvas: HTMLCanvasElement;
    private ctx: CanvasRenderingContext2D;
    private animationFrame: number;
    
    visualizeEmotions(
      internal: EmotionState,
      external: EmotionState,
      mode: 'ASD' | 'NT'
    ) {
      // å†…éƒ¨æ„Ÿæƒ…ã®è¡¨ç¤ºï¼ˆå¤§ããªå††ï¼‰
      this.drawEmotionCircle(
        this.canvas.width / 3,
        this.canvas.height / 2,
        100,
        internal,
        'internal'
      );
      
      // å¤–éƒ¨è¡¨ç¾ã®è¡¨ç¤ºï¼ˆå°ã•ãªå††ï¼‰
      this.drawEmotionCircle(
        (this.canvas.width / 3) * 2,
        this.canvas.height / 2,
        mode === 'ASD' ? 30 : 80,
        external,
        'external'
      );
      
      // æ¥ç¶šç·š
      this.drawConnection(internal, external, mode);
      
      // ãƒ©ãƒ™ãƒ«
      this.drawLabels(internal, external, mode);
      
      // ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
      this.animateTransition(internal, external);
    }
    
    private drawEmotionCircle(
      x: number,
      y: number,
      radius: number,
      emotion: EmotionState,
      type: 'internal' | 'external'
    ) {
      const gradient = this.ctx.createRadialGradient(x, y, 0, x, y, radius);
      const color = this.getEmotionColor(emotion);
      
      gradient.addColorStop(0, color.light);
      gradient.addColorStop(0.7, color.main);
      gradient.addColorStop(1, color.dark);
      
      this.ctx.fillStyle = gradient;
      this.ctx.beginPath();
      this.ctx.arc(x, y, radius * emotion.intensity, 0, Math.PI * 2);
      this.ctx.fill();
      
      // ãƒ‘ãƒ«ã‚¹ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
      if (type === 'internal') {
        this.ctx.strokeStyle = color.main;
        this.ctx.lineWidth = 2;
        this.ctx.beginPath();
        this.ctx.arc(
          x, y,
          radius * emotion.intensity + Math.sin(Date.now() * 0.002) * 10,
          0, Math.PI * 2
        );
        this.ctx.stroke();
      }
    }
    
    private getEmotionColor(emotion: EmotionState): EmotionColor {
      const colors = {
        joy: { light: '#FFE082', main: '#FFC107', dark: '#F57C00' },
        sadness: { light: '#90CAF9', main: '#2196F3', dark: '#0D47A1' },
        anger: { light: '#EF9A9A', main: '#F44336', dark: '#B71C1C' },
        fear: { light: '#CE93D8', main: '#9C27B0', dark: '#4A148C' },
        surprise: { light: '#80CBC4', main: '#009688', dark: '#004D40' },
        neutral: { light: '#E0E0E0', main: '#9E9E9E', dark: '#424242' }
      };
      
      return colors[emotion.primary] || colors.neutral;
    }
  }
  ```

### Day 109-110: æ„Ÿæƒ…ã®å¯è¦–åŒ–UI
- [ ] æ„Ÿæƒ…ãƒ¡ãƒ¼ã‚¿ãƒ¼å®Ÿè£…
  ```typescript
  const EmotionMeter: React.FC<{ emotion: SeparatedEmotions }> = ({ emotion }) => {
    const [animatedValues, setAnimatedValues] = useState({
      internal: 0,
      external: 0
    });
    
    useEffect(() => {
      // ã‚¹ãƒ ãƒ¼ã‚ºãªã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
      const animation = {
        internal: animatedValues.internal,
        external: animatedValues.external
      };
      
      const animationFrame = requestAnimationFrame(() => {
        animation.internal += (emotion.internal.intensity - animation.internal) * 0.1;
        animation.external += (emotion.external.intensity - animation.external) * 0.1;
        
        setAnimatedValues({ ...animation });
      });
      
      return () => cancelAnimationFrame(animationFrame);
    }, [emotion]);
    
    return (
      <div className="emotion-meter">
        <div className="meter-container">
          <h4>å†…éƒ¨æ„Ÿæƒ…</h4>
          <div className="meter-bar">
            <div 
              className="meter-fill internal"
              style={{ 
                width: `${animatedValues.internal * 100}%`,
                backgroundColor: getEmotionColor(emotion.internal.emotion)
              }}
            />
            <span className="meter-label">
              {emotion.internal.emotion.primary} 
              ({Math.round(emotion.internal.intensity * 100)}%)
            </span>
          </div>
        </div>
        
        <div className="meter-container">
          <h4>å¤–éƒ¨è¡¨ç¾</h4>
          <div className="meter-bar">
            <div 
              className="meter-fill external"
              style={{ 
                width: `${animatedValues.external * 100}%`,
                backgroundColor: getEmotionColor(emotion.external.emotion)
              }}
            />
            <span className="meter-label">
              {emotion.external.emotion.primary} 
              ({Math.round(emotion.external.intensity * 100)}%)
            </span>
          </div>
          
          {emotion.external.verbal && (
            <div className="verbal-expression">
              ğŸ’¬ {emotion.external.verbal.text}
            </div>
          )}
        </div>
        
        <div className="congruence-indicator">
          <span>ä¸€è‡´åº¦: {Math.round(emotion.external.congruence * 100)}%</span>
          <div className="congruence-bar">
            <div 
              className="congruence-fill"
              style={{ width: `${emotion.external.congruence * 100}%` }}
            />
          </div>
        </div>
      </div>
    );
  };
  ```
- [ ] ã‚°ãƒ©ãƒ•è¡¨ç¤º
  ```typescript
  const EmotionHistoryChart: React.FC<{ history: EmotionSnapshot[] }> = ({ history }) => {
    const chartRef = useRef<HTMLCanvasElement>(null);
    
    useEffect(() => {
      if (!chartRef.current) return;
      
      const ctx = chartRef.current.getContext('2d');
      const chart = new Chart(ctx, {
        type: 'line',
        data: {
          labels: history.map((_, i) => i),
          datasets: [
            {
              label: 'å†…éƒ¨æ„Ÿæƒ…',
              data: history.map(h => h.internal.intensity),
              borderColor: 'rgb(255, 99, 132)',
              backgroundColor: 'rgba(255, 99, 132, 0.1)',
              tension: 0.4
            },
            {
              label: 'å¤–éƒ¨è¡¨ç¾',
              data: history.map(h => h.external.intensity),
              borderColor: 'rgb(54, 162, 235)',
              backgroundColor: 'rgba(54, 162, 235, 0.1)',
              tension: 0.4
            }
          ]
        },
        options: {
          responsive: true,
          plugins: {
            legend: {
              position: 'top'
            },
            title: {
              display: true,
              text: 'æ„Ÿæƒ…ã®æ™‚ç³»åˆ—å¤‰åŒ–'
            },
            tooltip: {
              callbacks: {
                afterLabel: (context) => {
                  const snapshot = history[context.dataIndex];
                  return `æ„Ÿæƒ…: ${snapshot.internal.emotion.primary}`;
                }
              }
            }
          },
          scales: {
            y: {
              beginAtZero: true,
              max: 1,
              title: {
                display: true,
                text: 'å¼·åº¦'
              }
            },
            x: {
              title: {
                display: true,
                text: 'æ™‚é–“'
              }
            }
          }
        }
      });
      
      return () => chart.destroy();
    }, [history]);
    
    return <canvas ref={chartRef} />;
  };
  ```
- [ ] ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
  ```typescript
  class EmotionAnimationController {
    private animations: Map<string, Animation> = new Map();
    
    animateEmotionChange(
      from: EmotionState,
      to: EmotionState,
      duration: number = 1000
    ): AnimationHandle {
      const animationId = generateId();
      
      const animation = new Animation({
        from: from,
        to: to,
        duration: duration,
        easing: 'easeInOutCubic',
        onUpdate: (progress, current) => {
          this.updateEmotionDisplay(current);
          this.updateVRMExpression(current);
        },
        onComplete: () => {
          this.animations.delete(animationId);
        }
      });
      
      this.animations.set(animationId, animation);
      animation.start();
      
      return {
        id: animationId,
        pause: () => animation.pause(),
        resume: () => animation.resume(),
        cancel: () => {
          animation.cancel();
          this.animations.delete(animationId);
        }
      };
    }
    
    private updateEmotionDisplay(emotion: EmotionState) {
      // UIã®æ›´æ–°
      EventBus.emit('emotion:update', emotion);
      
      // ã‚«ãƒ©ãƒ¼ãƒ†ãƒ¼ãƒã®å¤‰æ›´
      this.updateColorTheme(emotion);
      
      // ãƒ‘ãƒ¼ãƒ†ã‚£ã‚¯ãƒ«ã‚¨ãƒ•ã‚§ã‚¯ãƒˆ
      if (emotion.intensity > 0.7) {
        this.triggerEmotionParticles(emotion);
      }
    }
    
    private updateVRMExpression(emotion: EmotionState) {
      const expressionMap = {
        joy: { happy: emotion.intensity, eyeSquint: emotion.intensity * 0.3 },
        sadness: { sad: emotion.intensity, eyeWide: emotion.intensity * 0.2 },
        anger: { angry: emotion.intensity, eyebrowAngry: emotion.intensity * 0.8 },
        fear: { surprised: emotion.intensity * 0.5, eyeWide: emotion.intensity },
        surprise: { surprised: emotion.intensity, eyeWide: emotion.intensity * 0.8 }
      };
      
      const expression = expressionMap[emotion.primary] || {};
      this.vrmManager.setExpression(expression);
    }
    
    private triggerEmotionParticles(emotion: EmotionState) {
      const particleConfig = {
        joy: {
          color: '#FFD700',
          shape: 'star',
          count: 20,
          velocity: { min: 1, max: 3 }
        },
        sadness: {
          color: '#4169E1',
          shape: 'teardrop',
          count: 10,
          velocity: { min: 0.5, max: 1 }
        },
        anger: {
          color: '#FF4500',
          shape: 'spike',
          count: 15,
          velocity: { min: 2, max: 4 }
        }
      };
      
      const config = particleConfig[emotion.primary];
      if (config) {
        this.particleSystem.emit(config);
      }
    }
  }
  ```

### Day 111-112: PCMåŸºæœ¬å®Ÿè£…ï¼ˆThinkerã‚¿ã‚¤ãƒ—ï¼‰
- [ ] Thinkerã‚¿ã‚¤ãƒ—å®Ÿè£…
  ```typescript
  class ThinkerPersonality implements PersonalityType {
    readonly type = 'Thinker';
    readonly characteristics = {
      primary: 'è«–ç†çš„æ€è€ƒ',
      strengths: ['åˆ†æåŠ›', 'å®¢è¦³æ€§', 'æ­£ç¢ºæ€§', 'ä½“ç³»çš„æ€è€ƒ'],
      communication: 'ãƒ‡ãƒ¼ã‚¿ã¨äº‹å®Ÿã«åŸºã¥ã',
      psychologicalNeeds: {
        recognition: 'ä»•äº‹ã®æˆæœã«å¯¾ã™ã‚‹èªè­˜',
        timeStructure: 'æ˜ç¢ºãªæ™‚é–“æ§‹é€ ã¨è¨ˆç”»'
      }
    };
    
    processInput(input: string, context: ConversationContext): ProcessedInput {
      // 1. è«–ç†çš„åˆ†æ
      const logicalAnalysis = this.analyzeLogically(input);
      
      // 2. äº‹å®Ÿã®æŠ½å‡º
      const facts = this.extractFacts(input);
      
      // 3. æ§‹é€ åŒ–
      const structured = this.structureInformation(logicalAnalysis, facts);
      
      // 4. æ„Ÿæƒ…ã¯æœ€å°é™ã«
      const emotionalContent = this.minimizeEmotionalContent(input);
      
      return {
        interpretation: structured.interpretation,
        response: this.generateLogicalResponse(structured),
        confidence: structured.confidence,
        reasoning: structured.reasoning,
        emotionalTone: 'neutral'
      };
    }
    
    generateResponse(
      processedInput: ProcessedInput,
      mode: 'ASD' | 'NT'
    ): PersonalityResponse {
      const baseResponse = {
        content: processedInput.response,
        structure: 'logical',
        emotionalIntensity: 0.2,
        communicationChannel: 'requestive'
      };
      
      if (mode === 'ASD') {
        // ASD + Thinker: æ¥µã‚ã¦è«–ç†çš„ã§æ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”
        return {
          ...baseResponse,
          content: this.addExtremeStructure(baseResponse.content),
          clarifications: this.generateDetailedClarifications(processedInput),
          references: this.addFactualReferences(processedInput),
          certaintyLevel: this.calculateCertainty(processedInput)
        };
      } else {
        // NT + Thinker: è«–ç†çš„ã ãŒæŸ”è»Ÿæ€§ã‚‚ã‚ã‚‹
        return {
          ...baseResponse,
          content: this.addModerateFlexibility(baseResponse.content),
          acknowledgments: this.acknowledgeOtherPerspectives(processedInput),
          suggestions: this.generateLogicalSuggestions(processedInput)
        };
      }
    }
    
    private structureInformation(
      analysis: LogicalAnalysis,
      facts: ExtractedFacts
    ): StructuredInfo {
      return {
        interpretation: `
          åˆ†æçµæœ:
          1. ä¸»è¦ãªäº‹å®Ÿ: ${facts.main.join(', ')}
          2. è«–ç†çš„é–¢ä¿‚: ${analysis.relationships.join(', ')}
          3. çµè«–: ${analysis.conclusion}
        `,
        confidence: analysis.certainty,
        reasoning: analysis.steps
      };
    }
    
    handleStress(level: StressLevel): StressBehavior {
      const behaviors = {
        1: {
          behavior: 'over-intellectualization',
          symptoms: ['éåº¦ã«è©³ç´°ãªèª¬æ˜', 'å®Œç’§ä¸»ç¾©ã®å¢—å¤§'],
          response: this.generateLevel1Response()
        },
        2: {
          behavior: 'condescending',
          symptoms: ['ä»–è€…ã®çŸ¥èƒ½ã‚’è»½è¦–', 'å‚²æ…¢ãªæ…‹åº¦'],
          response: this.generateLevel2Response()
        },
        3: {
          behavior: 'withdrawal',
          symptoms: ['å¼•ãã“ã‚‚ã‚Š', 'ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ‹’å¦'],
          response: this.generateLevel3Response()
        }
      };
      
      return behaviors[level];
    }
  }
  ```
- [ ] ãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£åˆ‡ã‚Šæ›¿ãˆ
  ```typescript
  class PersonalityManager {
    private personalities: Map<PCMType, PersonalityType> = new Map();
    private currentPersonality: PersonalityType;
    private transitionInProgress = false;
    
    constructor() {
      this.initializePersonalities();
    }
    
    private initializePersonalities() {
      this.personalities.set('Thinker', new ThinkerPersonality());
      // ä»–ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£ã¯å¾Œã§å®Ÿè£…
      this.personalities.set('Persister', new PlaceholderPersonality('Persister'));
      this.personalities.set('Harmonizer', new PlaceholderPersonality('Harmonizer'));
      this.personalities.set('Imaginer', new PlaceholderPersonality('Imaginer'));
      this.personalities.set('Rebel', new PlaceholderPersonality('Rebel'));
      this.personalities.set('Promoter', new PlaceholderPersonality('Promoter'));
      
      this.currentPersonality = this.personalities.get('Thinker')!;
    }
    
    async switchPersonality(
      newType: PCMType,
      context: TransitionContext
    ): Promise<TransitionResult> {
      if (this.transitionInProgress) {
        throw new Error('Personality transition already in progress');
      }
      
      this.transitionInProgress = true;
      
      try {
        const oldPersonality = this.currentPersonality;
        const newPersonality = this.personalities.get(newType);
        
        if (!newPersonality) {
          throw new Error(`Personality type ${newType} not found`);
        }
        
        // 1. ç§»è¡Œæº–å‚™
        const preparation = await this.prepareTransition(
          oldPersonality,
          newPersonality,
          context
        );
        
        // 2. çŠ¶æ…‹ã®ä¿å­˜
        const savedState = await this.savePersonalityState(oldPersonality);
        
        // 3. ç§»è¡Œã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
        await this.animateTransition(oldPersonality.type, newType);
        
        // 4. æ–°ã—ã„ãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£ã®åˆæœŸåŒ–
        await this.initializeNewPersonality(newPersonality, savedState);
        
        // 5. åˆ‡ã‚Šæ›¿ãˆå®Œäº†
        this.currentPersonality = newPersonality;
        
        return {
          success: true,
          fromType: oldPersonality.type,
          toType: newType,
          transitionDuration: preparation.duration,
          notes: preparation.notes
        };
      } finally {
        this.transitionInProgress = false;
      }
    }
    
    private async animateTransition(
      from: PCMType,
      to: PCMType
    ): Promise<void> {
      // VRMãƒ¢ãƒ‡ãƒ«ã®è¡¨æƒ…é·ç§»
      await this.vrmManager.transitionExpression(
        this.getBaseExpression(from),
        this.getBaseExpression(to),
        1000
      );
      
      // UIãƒ†ãƒ¼ãƒã®å¤‰æ›´
      await this.uiManager.transitionTheme(
        this.getTheme(from),
        this.getTheme(to),
        500
      );
      
      // éŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´
      await this.voiceManager.adjustParameters(
        this.getVoiceProfile(to)
      );
    }
  }
  ```
- [ ] ç‰¹æ€§ã®åæ˜ 
  ```typescript
  class ThinkerTraitImplementation {
    applyTraits(
      message: Message,
      mode: 'ASD' | 'NT'
    ): TraitApplicationResult {
      const traits = {
        // è«–ç†çš„æ€è€ƒã®é©ç”¨
        logicalThinking: this.applyLogicalThinking(message),
        
        // ãƒ‡ãƒ¼ã‚¿é‡è¦–ã®é©ç”¨
        dataFocus: this.applyDataFocus(message),
        
        // æ§‹é€ åŒ–ã®é©ç”¨
        structuring: this.applyStructuring(message),
        
        // æ„Ÿæƒ…ã®æœ€å°åŒ–
        emotionalMinimization: this.minimizeEmotions(message)
      };
      
      // ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®èª¿æ•´
      if (mode === 'ASD') {
        traits.logicalThinking.intensity *= 1.5;
        traits.structuring.level = 'extreme';
        traits.emotionalMinimization.level = 'maximum';
      } else {
        traits.logicalThinking.intensity *= 1.2;
        traits.structuring.level = 'moderate';
        traits.emotionalMinimization.level = 'balanced';
      }
      
      return {
        modifiedMessage: this.applyAllTraits(message, traits),
        appliedTraits: traits,
        confidence: this.calculateConfidence(traits)
      };
    }
    
    private applyLogicalThinking(message: Message): LogicalThinkingResult {
      // è«–ç†çš„ãªæ¥ç¶šè©ã®ä½¿ç”¨
      const logicalConnectors = [
        'ã—ãŸãŒã£ã¦', 'ã‚†ãˆã«', 'çµæœã¨ã—ã¦',
        'ãªãœãªã‚‰', 'ã¤ã¾ã‚Š', 'è¦ã™ã‚‹ã«'
      ];
      
      // è«–ç†æ§‹é€ ã®åˆ†æ
      const structure = this.analyzeLogicalStructure(message.content);
      
      // è«–ç†çš„ãªå†æ§‹æˆ
      const restructured = this.restructureLogically(
        message.content,
        structure,
        logicalConnectors
      );
      
      return {
        original: message.content,
        restructured: restructured,
        logicalFlow: structure.flow,
        intensity: structure.coherence
      };
    }
    
    private applyDataFocus(message: Message): DataFocusResult {
      // æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®å¼·èª¿
      const numbers = this.extractNumbers(message.content);
      const statistics = this.extractStatistics(message.content);
      
      // äº‹å®Ÿã®å„ªå…ˆ
      const facts = this.prioritizeFacts(message.content);
      
      // å®¢è¦³çš„ãªè¡¨ç¾ã¸ã®å¤‰æ›
      const objectified = this.convertToObjective(message.content);
      
      return {
        dataPoints: [...numbers, ...statistics],
        factualContent: facts,
        objectiveExpression: objectified,
        dataRichness: this.calculateDataRichness(numbers, statistics, facts)
      };
    }
  }
  ```

### Day 113-114: ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«è¡¨ç¤º
- [ ] ã‚¹ãƒˆãƒ¬ã‚¹è¨ˆç®—
  ```typescript
  class StressCalculator {
    private stressFactors: StressFactor[] = [];
    private currentLevel: number = 0;
    private history: StressSnapshot[] = [];
    
    calculateStress(
      context: ConversationContext,
      personality: PersonalityType,
      mode: 'ASD' | 'NT'
    ): StressLevel {
      // åŸºæœ¬ã‚¹ãƒˆãƒ¬ã‚¹è¦å› 
      const factors = {
        ambiguity: this.calculateAmbiguityStress(context),
        socialDemand: this.calculateSocialStress(context),
        sensoryLoad: this.calculateSensoryStress(context),
        unpredictability: this.calculateUnpredictabilityStress(context),
        timePresure: this.calculateTimePressure(context)
      };
      
      // ãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£åˆ¥ã®é‡ã¿ä»˜ã‘
      const weights = this.getStressWeights(personality.type);
      
      // ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®èª¿æ•´
      const modeMultiplier = mode === 'ASD' ? 1.5 : 1.0;
      
      // ç·åˆã‚¹ãƒˆãƒ¬ã‚¹å€¤ã®è¨ˆç®—
      let totalStress = 0;
      for (const [factor, value] of Object.entries(factors)) {
        totalStress += value * weights[factor] * modeMultiplier;
      }
      
      // 0-1ã®ç¯„å›²ã«æ­£è¦åŒ–
      totalStress = Math.min(1, Math.max(0, totalStress));
      
      // ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«ã®åˆ¤å®šï¼ˆ1-3ï¼‰
      const level = this.determineStressLevel(totalStress);
      
      // å±¥æ­´ã®æ›´æ–°
      this.updateHistory({
        timestamp: Date.now(),
        level: level,
        factors: factors,
        totalStress: totalStress
      });
      
      return {
        level: level,
        value: totalStress,
        factors: factors,
        trend: this.calculateTrend(),
        recommendations: this.generateRecommendations(level, factors)
      };
    }
    
    private calculateAmbiguityStress(context: ConversationContext): number {
      let stress = 0;
      
      // æ›–æ˜§ãªè¡¨ç¾ã®æ¤œå‡º
      const ambiguousTerms = [
        'ã¡ã‚‡ã£ã¨', 'é©å½“', 'ãã‚Œãªã‚Š', 'ã„ã„æ„Ÿã˜',
        'ãã®ã†ã¡', 'ãŸã¶ã‚“', 'ã¿ãŸã„ãª'
      ];
      
      const message = context.lastMessage?.content || '';
      ambiguousTerms.forEach(term => {
        if (message.includes(term)) {
          stress += 0.2;
        }
      });
      
      // æ–‡è„ˆã®ä¸æ˜ç¢ºã•
      if (!context.topic || context.topic === 'unknown') {
        stress += 0.3;
      }
      
      return Math.min(1, stress);
    }
    
    private determineStressLevel(stress: number): 1 | 2 | 3 {
      if (stress < 0.3) return 1;
      if (stress < 0.7) return 2;
      return 3;
    }
  }
  ```
- [ ] æ®µéšè¡¨ç¤º
  ```typescript
  const StressIndicator: React.FC<{ stressData: StressLevel }> = ({ stressData }) => {
    const getStressColor = (level: number) => {
      const colors = {
        1: '#4CAF50', // ç·‘
        2: '#FF9800', // ã‚ªãƒ¬ãƒ³ã‚¸
        3: '#F44336'  // èµ¤
      };
      return colors[level];
    };
    
    const getStressIcon = (level: number) => {
      const icons = {
        1: 'ğŸ˜Œ', // ãƒªãƒ©ãƒƒã‚¯ã‚¹
        2: 'ğŸ˜°', // ä¸å®‰
        3: 'ğŸ˜–'  // é«˜ã‚¹ãƒˆãƒ¬ã‚¹
      };
      return icons[level];
    };
    
    return (
      <div className="stress-indicator">
        <div className="stress-header">
          <h4>ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«</h4>
          <span className="stress-icon">{getStressIcon(stressData.level)}</span>
        </div>
        
        <div className="stress-meter">
          <div className="stress-segments">
            {[1, 2, 3].map(level => (
              <div
                key={level}
                className={`stress-segment ${stressData.level >= level ? 'active' : ''}`}
                style={{
                  backgroundColor: stressData.level >= level 
                    ? getStressColor(level) 
                    : '#e0e0e0'
                }}
              />
            ))}
          </div>
          
          <div className="stress-value">
            {Math.round(stressData.value * 100)}%
          </div>
        </div>
        
        <div className="stress-factors">
          <h5>ã‚¹ãƒˆãƒ¬ã‚¹è¦å› </h5>
          {Object.entries(stressData.factors).map(([factor, value]) => (
            <div key={factor} className="factor-item">
              <span className="factor-name">
                {getFactorLabel(factor)}
              </span>
              <div className="factor-bar">
                <div 
                  className="factor-fill"
                  style={{ 
                    width: `${value * 100}%`,
                    backgroundColor: getStressColor(Math.ceil(value * 3))
                  }}
                />
              </div>
            </div>
          ))}
        </div>
        
        {stressData.recommendations && (
          <div className="stress-recommendations">
            <h5>æ¨å¥¨ã•ã‚Œã‚‹å¯¾å‡¦æ³•</h5>
            <ul>
              {stressData.recommendations.map((rec, i) => (
                <li key={i}>{rec}</li>
              ))}
            </ul>
          </div>
        )}
        
        <StressTrendChart history={stressData.history} />
      </div>
    );
  };
  ```
- [ ] è¡Œå‹•å¤‰åŒ–
  ```typescript
  class StressBehaviorManager {
    applyStressBehavior(
      personality: PersonalityType,
      stressLevel: 1 | 2 | 3,
      baseResponse: string
    ): StressModifiedResponse {
      const behaviorModifiers = {
        Thinker: {
          1: (response) => this.addExcessiveDetail(response),
          2: (response) => this.addCondescension(response),
          3: (response) => this.minimizeResponse(response)
        },
        Persister: {
          1: (response) => this.addOpinionPushing(response),
          2: (response) => this.addCriticism(response),
          3: (response) => this.addDespair(response)
        },
        Harmonizer: {
          1: (response) => this.addExcessiveCare(response),
          2: (response) => this.addSelfBlame(response),
          3: (response) => this.addEmotionalOverload(response)
        }
        // ä»–ã®ã‚¿ã‚¤ãƒ—ã¯å¾Œã§å®Ÿè£…
      };
      
      const modifier = behaviorModifiers[personality.type]?.[stressLevel];
      if (!modifier) {
        return { 
          content: baseResponse, 
          stressModified: false 
        };
      }
      
      const modifiedResponse = modifier(baseResponse);
      
      // ã‚¹ãƒˆãƒ¬ã‚¹è¡¨ç¾ã®è¿½åŠ 
      const stressExpressions = this.getStressExpressions(
        personality.type,
        stressLevel
      );
      
      // éè¨€èªçš„ãªå¤‰åŒ–
      const nonverbalChanges = this.getNonverbalChanges(stressLevel);
      
      return {
        content: modifiedResponse,
        stressModified: true,
        stressLevel: stressLevel,
        expressions: stressExpressions,
        nonverbal: nonverbalChanges,
        warning: stressLevel >= 2 
          ? 'ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«ãŒé«˜ã¾ã£ã¦ã„ã¾ã™' 
          : null
      };
    }
    
    private addExcessiveDetail(response: string): string {
      // Thinker Level 1: éåº¦ã«è©³ç´°ãªèª¬æ˜
      const additions = [
        'è©³ç´°ã«èª¬æ˜ã—ã¾ã™ã¨ã€',
        'ã‚ˆã‚Šæ­£ç¢ºã«è¨€ãˆã°ã€',
        'è£œè¶³ã¨ã—ã¦ä»˜ã‘åŠ ãˆã¾ã™ãŒã€',
        'ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦è¿°ã¹ã‚‹ã¨ã€'
      ];
      
      // æ–‡ã‚’åˆ†å‰²ã—ã¦å„éƒ¨åˆ†ã«è©³ç´°ã‚’è¿½åŠ 
      const sentences = response.split('ã€‚');
      const detailed = sentences.map(sentence => {
        if (sentence.trim()) {
          const addition = additions[Math.floor(Math.random() * additions.length)];
          return `${sentence}ã€‚${addition}${this.generateDetail(sentence)}`;
        }
        return sentence;
      }).join('ã€‚');
      
      return detailed;
    }
    
    private addCondescension(response: string): string {
      // Thinker Level 2: è¦‹ä¸‹ã™ã‚ˆã†ãªæ…‹åº¦
      const condescendingPhrases = [
        'ã”å­˜çŸ¥ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€',
        'åŸºæœ¬çš„ãªã“ã¨ã§ã™ãŒã€',
        'å½“ç„¶ã®ã“ã¨ãªãŒã‚‰ã€',
        'ç†è§£ã§ããªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€'
      ];
      
      const phrase = condescendingPhrases[
        Math.floor(Math.random() * condescendingPhrases.length)
      ];
      
      return `${phrase}${response}`;
    }
  }
  ```

### Day 115-118: æœ€é©åŒ–ã¨çµ±åˆ
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
  ```typescript
  class Phase5PerformanceOptimizer {
    optimizeAudioProcessing() {
      // Web Audio APIã®æœ€é©åŒ–
      const audioOptimizations = {
        // ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®äº‹å‰å‡¦ç†
        preProcessAudio: async (audioBuffer: AudioBuffer) => {
          const offlineContext = new OfflineAudioContext(
            audioBuffer.numberOfChannels,
            audioBuffer.length,
            audioBuffer.sampleRate
          );
          
          // å‡¦ç†ãƒãƒ¼ãƒ‰ã®è¨­å®š
          const source = offlineContext.createBufferSource();
          source.buffer = audioBuffer;
          
          // ã‚³ãƒ³ãƒ—ãƒ¬ãƒƒã‚µãƒ¼
          const compressor = offlineContext.createDynamicsCompressor();
          compressor.threshold.value = -24;
          compressor.ratio.value = 4;
          
          source.connect(compressor);
          compressor.connect(offlineContext.destination);
          source.start();
          
          return offlineContext.startRendering();
        },
        
        // ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªãƒãƒƒãƒ•ã‚¡ã®å†åˆ©ç”¨
        bufferPool: new ObjectPool<AudioBuffer>({
          create: () => new AudioBuffer({ 
            numberOfChannels: 2, 
            length: 48000, 
            sampleRate: 48000 
          }),
          reset: (buffer) => { /* ãƒãƒƒãƒ•ã‚¡ã®ã‚¯ãƒªã‚¢ */ },
          maxSize: 10
        })
      };
      
      return audioOptimizations;
    }
    
    optimizeEmotionCalculations() {
      // æ„Ÿæƒ…è¨ˆç®—ã®æœ€é©åŒ–
      return {
        // ãƒ¡ãƒ¢åŒ–
        memoizedCalculations: new Map(),
        
        // ãƒãƒƒãƒå‡¦ç†
        batchEmotionUpdates: (updates: EmotionUpdate[]) => {
          return updates.reduce((acc, update) => {
            // ä¼¼ãŸæ„Ÿæƒ…ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            const key = `${update.type}-${Math.round(update.intensity * 10)}`;
            if (!acc[key]) {
              acc[key] = [];
            }
            acc[key].push(update);
            return acc;
          }, {});
        },
        
        // å·®åˆ†æ›´æ–°ã®ã¿
        deltaUpdates: (previous: EmotionState, current: EmotionState) => {
          const delta = {};
          for (const key in current) {
            if (Math.abs(current[key] - previous[key]) > 0.01) {
              delta[key] = current[key];
            }
          }
          return delta;
        }
      };
    }
  }
  ```
- [ ] ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ
  ```typescript
  describe('Phase 5 çµ±åˆãƒ†ã‚¹ãƒˆ', () => {
    let voiceSystem: VoiceSystem;
    let emotionSystem: EmotionSystem;
    let pcmSystem: PCMSystem;
    
    beforeEach(() => {
      voiceSystem = new VoiceSystem();
      emotionSystem = new EmotionSystem();
      pcmSystem = new PCMSystem();
    });
    
    test('éŸ³å£°åˆæˆã¨æ„Ÿæƒ…è¡¨ç¾ã®åŒæœŸ', async () => {
      const text = 'ã¨ã¦ã‚‚å¬‰ã—ã„ã§ã™';
      const emotion = { type: 'joy', intensity: 0.8 };
      
      // æ„Ÿæƒ…è§£æ
      const emotionResult = await emotionSystem.analyze(text, 'ASD');
      expect(emotionResult.internal.intensity).toBe(0.8);
      expect(emotionResult.external.intensity).toBeLessThan(0.3);
      
      // éŸ³å£°åˆæˆ
      const voiceParams = voiceSystem.adjustForEmotion(
        baseParams,
        emotionResult.external
      );
      const audio = await voiceSystem.synthesize(text, voiceParams);
      
      expect(audio).toBeDefined();
      expect(voiceParams.pitchScale).toBeLessThan(1.1); // ASDãƒ¢ãƒ¼ãƒ‰ã§ã¯æŠ‘åˆ¶
    });
    
    test('PCMãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£ã¨ã‚¹ãƒˆãƒ¬ã‚¹ã®é€£å‹•', async () => {
      const personality = pcmSystem.setPersonality('Thinker');
      const stressFactors = {
        ambiguity: 0.8,
        socialDemand: 0.6
      };
      
      const stressLevel = await pcmSystem.calculateStress(stressFactors);
      expect(stressLevel).toBe(2); // ä¸­ç¨‹åº¦ã®ã‚¹ãƒˆãƒ¬ã‚¹
      
      const response = await personality.generateResponse(
        'ã¡ã‚‡ã£ã¨å¾…ã£ã¦',
        { stressLevel }
      );
      
      expect(response).toContain('å…·ä½“çš„ã«'); // ã‚¹ãƒˆãƒ¬ã‚¹æ™‚ã®è©³ç´°è¦æ±‚
    });
  });
  ```
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
  ```markdown
  # Phase 5: éŸ³å£°ãƒ»æ„Ÿæƒ…ã‚·ã‚¹ãƒ†ãƒ  å®Ÿè£…ã‚¬ã‚¤ãƒ‰
  
  ## æ¦‚è¦
  Phase 5ã§ã¯ã€ã‚ˆã‚Šè‡ªç„¶ã§è±Šã‹ãªå¯¾è©±ä½“é¨“ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã€
  éŸ³å£°åˆæˆã‚·ã‚¹ãƒ†ãƒ ã¨é«˜åº¦ãªæ„Ÿæƒ…è¡¨ç¾ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚
  
  ## ä¸»è¦æ©Ÿèƒ½
  
  ### 1. éŸ³å£°åˆæˆï¼ˆVOICEVOXçµ±åˆï¼‰
  - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ç”Ÿæˆ
  - ãƒ¢ãƒ¼ãƒ‰åˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
  - ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯å¯¾å¿œ
  
  ### 2. æ„Ÿæƒ…ã‚·ã‚¹ãƒ†ãƒ 
  - å†…éƒ¨æ„Ÿæƒ…ã¨å¤–éƒ¨è¡¨ç¾ã®åˆ†é›¢
  - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ„Ÿæƒ…æ¨ç§»
  - å¯è¦–åŒ–UI
  
  ### 3. PCMãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£
  - Thinkerã‚¿ã‚¤ãƒ—å®Ÿè£…
  - ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«ç®¡ç†
  - è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³å¤‰åŒ–
  
  ## ä½¿ç”¨æ–¹æ³•
  
  ### éŸ³å£°åˆæˆã®åˆ©ç”¨
  ```typescript
  const voiceClient = new VoiceVoxClient();
  const audio = await voiceClient.synthesize('ã“ã‚“ã«ã¡ã¯', {
    speaker: 3,
    speed: 0.95,
    pitch: 1.0
  });
  ```
  
  ### æ„Ÿæƒ…åˆ†æã®å®Ÿè¡Œ
  ```typescript
  const emotion = await emotionEngine.analyze(text, mode);
  console.log(`å†…éƒ¨æ„Ÿæƒ…: ${emotion.internal.intensity}`);
  console.log(`å¤–éƒ¨è¡¨ç¾: ${emotion.external.intensity}`);
  ```
  
  ## è¨­å®šé …ç›®
  
  ### éŸ³å£°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  - `speedScale`: è©±é€Ÿï¼ˆ0.5-2.0ï¼‰
  - `pitchScale`: éŸ³é«˜ï¼ˆ0.5-2.0ï¼‰  
  - `intonationScale`: æŠ‘æšï¼ˆ0.0-2.0ï¼‰
  - `volumeScale`: éŸ³é‡ï¼ˆ0.0-2.0ï¼‰
  
  ### æ„Ÿæƒ…ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
  - `expressionRatio`: è¡¨ç¾ç‡ï¼ˆASD: 0.3, NT: 0.8ï¼‰
  - `emotionInertia`: æ„Ÿæƒ…ã®æ…£æ€§ï¼ˆ0.0-1.0ï¼‰
  - `verbalizationThreshold`: è¨€èªåŒ–é–¾å€¤
  ```

## æˆæœç‰©ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### éŸ³å£°æ©Ÿèƒ½
- [ ] VOICEVOX APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…
- [ ] è©±è€…é¸æŠãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
- [ ] éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
- [ ] ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯åŒæœŸ
- [ ] éŸ³å£°å…¥åŠ›ï¼ˆWeb Speech APIï¼‰

### æ„Ÿæƒ…è¡¨ç¾
- [ ] å†…éƒ¨æ„Ÿæƒ…vså¤–éƒ¨è¡¨ç¾ã®åˆ†é›¢ãƒ­ã‚¸ãƒƒã‚¯
- [ ] æ„Ÿæƒ…å¯è¦–åŒ–UIï¼ˆãƒ¡ãƒ¼ã‚¿ãƒ¼ã€ã‚°ãƒ©ãƒ•ï¼‰
- [ ] æ„Ÿæƒ…ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
- [ ] ãƒ¢ãƒ¼ãƒ‰åˆ¥æ„Ÿæƒ…è¡¨ç¾

### PCMã‚·ã‚¹ãƒ†ãƒ 
- [ ] Thinkerãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£å®Ÿè£…
- [ ] ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«è¨ˆç®—ãƒ»è¡¨ç¤º
- [ ] ã‚¹ãƒˆãƒ¬ã‚¹è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
- [ ] ãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£åˆ‡ã‚Šæ›¿ãˆæ©Ÿèƒ½

### çµ±åˆ
- [ ] éŸ³å£°ãƒ»æ„Ÿæƒ…ãƒ»VRMã®åŒæœŸ
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ

## ãƒªã‚¹ã‚¯ã¨å¯¾ç­–

| ãƒªã‚¹ã‚¯ | å¯¾ç­– |
|--------|------|
| VOICEVOXæ¥ç¶šã‚¨ãƒ©ãƒ¼ | ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯éŸ³å£°ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° |
| ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯é…å»¶ | äº‹å‰è¨ˆç®—ã€ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚° |
| æ„Ÿæƒ…è¨ˆç®—ã®è² è· | ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã€å·®åˆ†æ›´æ–° |
| ã‚¹ãƒˆãƒ¬ã‚¹è¡¨ç¾ã®ä¸è‡ªç„¶ã• | æ®µéšçš„ãªå¤‰åŒ–ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ†ã‚¹ãƒˆ |

## æ¬¡ã®ãƒ•ã‚§ãƒ¼ã‚ºã¸ã®æº–å‚™

Phase 6ï¼ˆå“è³ªå‘ä¸Šï¼‰ã«å‘ã‘ã¦ï¼š
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åé›†
- [ ] ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£èª²é¡Œãƒªã‚¹ãƒˆä½œæˆ
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»æº–å‚™
- [ ] æœ¬ç•ªç’°å¢ƒè¦ä»¶ç¢ºèª