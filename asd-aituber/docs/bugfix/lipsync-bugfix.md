# ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯å•é¡Œ å®Œå…¨è§£æãƒ¬ãƒãƒ¼ãƒˆ ğŸ”

**èª¿æŸ»æ—¥**: 2025-06-09  
**ç—‡çŠ¶**: éŸ³å£°ç™ºè©±ã¨ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ãŒå…¨ãåŒæœŸã›ãšã€å£ãŒå‹•ã‹ãªã„çŠ¶æ…‹  
**èª¿æŸ»å¯¾è±¡**: asd-aituber vs aituber-kit ã®å®Ÿè£…æ¯”è¼ƒ  

---

## ğŸš¨ å•é¡Œã®æ ¸å¿ƒ

ç¾åœ¨ã®asd-aituberã®ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯å®Ÿè£…ã¯**æ ¹æœ¬çš„ã«è¨­è¨ˆãŒé–“é•ã£ã¦ã„ã¾ã™**ã€‚

### ç¾åœ¨ã®å®Ÿè£…ï¼ˆéå‹•ä½œï¼‰
```typescript
// âŒ å•é¡Œã®ã‚ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼šãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹æ“¬ä¼¼åŒæœŸ
1. HTMLAudioElement ã§éŸ³å£°å†ç”Ÿ
2. requestAnimationFrame ã§ç‹¬ç«‹ã—ãŸã‚¿ã‚¤ãƒãƒ¼å‹•ä½œ 
3. æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®æ™‚é–“è¨ˆç®—ã§ãƒ•ã‚§ã‚¤ã‚¯ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯
4. 3ã¤ã®ã‚·ã‚¹ãƒ†ãƒ ãŒç‹¬ç«‹ã—ã¦å‹•ä½œï¼ˆåŒæœŸä¸å¯èƒ½ï¼‰
```

### ä½œå‹•ã™ã‚‹aituber-kitå®Ÿè£…
```typescript
// âœ… æ­£ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°è§£æ
1. AudioContext ã§éŸ³å£°å†ç”Ÿ
2. AnalyserNode ã§å®Ÿéš›ã®éŸ³å£°ãƒœãƒªãƒ¥ãƒ¼ãƒ å–å¾—
3. 60fps ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«å£ã®é–‹ãå…·åˆã‚’åˆ¶å¾¡
4. éŸ³å£°ã¨å£ãƒ‘ã‚¯ãŒç‰©ç†çš„ã«æ¥ç¶š
```

---

## ğŸ”§ æŠ€è¡“çš„æ ¹æœ¬åŸå› åˆ†æ

### 1. **è‡´å‘½çš„å•é¡Œ: éŸ³å£°æ¥ç¶šã®æ¬ å¦‚**

#### aituber-kitï¼ˆå‹•ä½œã™ã‚‹ï¼‰
```typescript
// aituber-kit/src/features/lipSync/lipSync.ts
export class LipSync {
  public readonly audio: AudioContext
  public readonly analyser: AnalyserNode
  public readonly timeDomainData: Float32Array

  constructor(audio: AudioContext) {
    this.audio = audio
    this.analyser = audio.createAnalyser()
    // âœ… å®Ÿéš›ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è§£æ
  }

  public update(): LipSyncAnalyzeResult {
    this.analyser.getFloatTimeDomainData(this.timeDomainData)
    
    let volume = 0.0
    for (let i = 0; i < TIME_DOMAIN_DATA_LENGTH; i++) {
      volume = Math.max(volume, Math.abs(this.timeDomainData[i]))
    }
    
    // ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§éŸ³é‡ã‚’èª¿æ•´
    volume = 1 / (1 + Math.exp(-45 * volume + 5))
    if (volume < 0.1) volume = 0
    
    return { volume } // âœ… å®Ÿéš›ã®éŸ³å£°ãƒœãƒªãƒ¥ãƒ¼ãƒ 
  }
}
```

#### asd-aituberï¼ˆå‹•ä½œã—ãªã„ï¼‰
```typescript
// asd-aituber/apps/web/lib/lip-sync.ts
export class LipSync {
  // âŒ AudioContext ãªã—
  // âŒ AnalyserNode ãªã—  
  // âŒ å®Ÿéš›ã®éŸ³å£°è§£æãªã—
  
  play(phonemes: PhonemeData[], onPhonemeChange: (phoneme: string, intensity: number) => void) {
    const animate = (currentTime: number) => {
      const elapsedTime = currentTime - startTime
      // âŒ æ–‡å­—æ•° Ã— 100ms ã®æ“¬ä¼¼ã‚¿ã‚¤ãƒŸãƒ³ã‚°
      // âŒ å®Ÿéš›ã®éŸ³å£°ã¨ã¯ç„¡é–¢ä¿‚
      const progress = elapsedTime / (phonemes.length * 100)
      this.onPhonemeChange(currentPhoneme.phoneme, 0.7) // âŒ å›ºå®šå€¤
    }
    requestAnimationFrame(animate)
  }
}
```

### 2. **è¡¨æƒ…åˆ¶å¾¡ã®ç«¶åˆå•é¡Œ**

#### asd-aituber ã®å•é¡Œã‚³ãƒ¼ãƒ‰
```typescript
// asd-aituber/apps/web/lib/vrm-animation.ts:547-577
private setStandardMouthExpression(phoneme: string, intensity: number): void {
  const expressionManager = this.vrm.expressionManager!
  const available = this.vowelExpressionInfo!.available
  
  // âŒ æ¯ãƒ•ãƒ¬ãƒ¼ãƒ å…¨ã¦ã®å£è¡¨æƒ…ã‚’ãƒªã‚»ãƒƒãƒˆ
  const allMouthExpressions = ['aa', 'ih', 'ou', 'ee', 'oh']
  allMouthExpressions.forEach(expr => {
    if (available.includes(expr)) {
      try {
        expressionManager.setValue(expr, 0) // âŒ ã™ã¹ã¦0ã«ãƒªã‚»ãƒƒãƒˆ
      } catch (error) {
        // Skip silently
      }
    }
  })

  // å£è¡¨æƒ…ã‚’è¨­å®šã—ã‚ˆã†ã¨ã™ã‚‹
  if (phoneme !== 'silence' && available.includes('aa')) {
    try {
      expressionManager.setValue('aa', intensity) // âŒ ä¸Šã§ãƒªã‚»ãƒƒãƒˆæ¸ˆã¿ãªã®ã§ç„¡åŠ¹
    } catch (error) {
      console.error(`Error setting 'aa' expression:`, error)
    }
  }
}
```

**å•é¡Œ**: æ¯ãƒ•ãƒ¬ãƒ¼ãƒ è¡¨æƒ…ã‚’ãƒªã‚»ãƒƒãƒˆ â†’ è¨­å®š â†’ ãƒªã‚»ãƒƒãƒˆãŒç¹°ã‚Šè¿”ã•ã‚Œã€çµæœçš„ã«å£ãŒå‹•ã‹ãªã„

#### aituber-kit ã®æ­£ã—ã„å®Ÿè£…
```typescript
// aituber-kit/src/features/emoteController/expressionController.ts
public lipSync(preset: VRMExpressionPresetName, value: number) {
  // âœ… å‰ã®å£è¡¨æƒ…ã®ã¿ã‚¯ãƒªã‚¢
  if (this._currentLipSync) {
    this._expressionManager?.setValue(this._currentLipSync.preset, 0)
  }
  // âœ… æ–°ã—ã„å£è¡¨æƒ…ã‚’è¨­å®š
  this._currentLipSync = { preset, value }
}

public update(delta: number) {
  if (this._currentLipSync) {
    const weight = this._currentEmotion === 'neutral'
      ? this._currentLipSync.value * 0.5 // âœ… æ„Ÿæƒ…ã«å¿œã˜ãŸé‡ã¿èª¿æ•´
      : this._currentLipSync.value * 0.25
    this._expressionManager?.setValue(this._currentLipSync.preset, weight) // âœ… å®Ÿéš›ã«å£ãŒå‹•ã
  }
}
```

### 3. **éŸ³å£°å†ç”Ÿæ–¹æ³•ã®é•ã„**

#### aituber-kitï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æå¯èƒ½ï¼‰
```typescript
// aituber-kit/src/features/messages/speakCharacter.ts
const bufferSource = this.audio.createBufferSource()
bufferSource.buffer = audioBuffer

// âœ… éŸ³å£°ã‚’è§£æå™¨ã«ã‚‚æ¥ç¶š
bufferSource.connect(this.destination)
bufferSource.connect(this.lipSync.analyser) // â† ã“ã“ãŒé‡è¦ï¼

bufferSource.start()
this.lipSync.start() // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æé–‹å§‹
```

#### asd-aituberï¼ˆè§£æä¸å¯èƒ½ï¼‰
```typescript
// asd-aituber/apps/web/lib/unified-voice-synthesis.ts
this.currentAudio = new Audio(audioUrl) // âŒ HTMLAudioElement
this.currentAudio.volume = volume

// âŒ AnalyserNode ã¸ã®æ¥ç¶šãªã—
// âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°è§£æä¸å¯èƒ½
await this.currentAudio.play()
```

---

## ğŸ“Š æ¯”è¼ƒè¡¨ï¼šå‹•ä½œã™ã‚‹ vs å‹•ä½œã—ãªã„

| è¦ç´  | aituber-kitï¼ˆâœ…å‹•ä½œï¼‰ | asd-aituberï¼ˆâŒä¸å‹•ä½œï¼‰ |
|------|---------------------|----------------------|
| **éŸ³å£°ã‚·ã‚¹ãƒ†ãƒ ** | AudioContext + AnalyserNode | HTMLAudioElement |
| **åŒæœŸæ–¹æ³•** | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³é‡è§£æ | ãƒ†ã‚­ã‚¹ãƒˆé•·ãƒ™ãƒ¼ã‚¹æ“¬ä¼¼ã‚¿ã‚¤ãƒŸãƒ³ã‚° |
| **æ›´æ–°é »åº¦** | 60fpsï¼ˆæ¯ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ | requestAnimationFrameï¼ˆä¸è¦å‰‡ï¼‰ |
| **å£è¡¨æƒ…åˆ¶å¾¡** | å˜ä¸€è¡¨æƒ…ã®é‡ã¿åˆ¶å¾¡ | å…¨è¡¨æƒ…ãƒªã‚»ãƒƒãƒˆ+è¨­å®šã®ç«¶åˆ |
| **éŸ³å£°æ¥ç¶š** | bufferSource.connect(analyser) | æ¥ç¶šãªã— |
| **ãƒœãƒªãƒ¥ãƒ¼ãƒ å–å¾—** | getFloatTimeDomainData() | å–å¾—ä¸å¯ |
| **çµæœ** | å®Œç’§ã«åŒæœŸ | å…¨ãå‹•ã‹ãªã„ |

---

## ğŸ¯ è§£æ±ºç­–: aituber-kitæ–¹å¼ã¸ã®ç§»è¡Œ

### Phase 1: AudioContext ãƒ™ãƒ¼ã‚¹ã®éŸ³å£°ã‚·ã‚¹ãƒ†ãƒ 

```typescript
// æ–°ã—ã„éŸ³å£°ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚¯ãƒ©ã‚¹
export class AudioLipSync {
  private audioContext: AudioContext
  private analyser: AnalyserNode
  private currentSource: AudioBufferSourceNode | null = null
  private timeDomainData: Float32Array
  
  constructor() {
    this.audioContext = new AudioContext()
    this.analyser = this.audioContext.createAnalyser()
    this.analyser.fftSize = 2048
    this.timeDomainData = new Float32Array(this.analyser.fftSize)
  }
  
  async playWithLipSync(audioBuffer: ArrayBuffer): Promise<void> {
    // AudioBuffer ã«ãƒ‡ã‚³ãƒ¼ãƒ‰
    const decodedBuffer = await this.audioContext.decodeAudioData(audioBuffer)
    
    // BufferSource ã‚’ä½œæˆ
    const source = this.audioContext.createBufferSource()
    source.buffer = decodedBuffer
    
    // âœ… é‡è¦: å‡ºåŠ›ã¨è§£æã®ä¸¡æ–¹ã«æ¥ç¶š
    source.connect(this.audioContext.destination) // ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼å‡ºåŠ›
    source.connect(this.analyser)                 // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è§£æ
    
    // å†ç”Ÿé–‹å§‹
    source.start()
    this.currentSource = source
  }
  
  // aituber-kit ã¨åŒã˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
  getVolume(): number {
    this.analyser.getFloatTimeDomainData(this.timeDomainData)
    
    let volume = 0.0
    for (let i = 0; i < this.timeDomainData.length; i++) {
      volume = Math.max(volume, Math.abs(this.timeDomainData[i]))
    }
    
    // ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å¤‰æ›ï¼ˆaituber-kit ã¨åŒã˜ï¼‰
    volume = 1 / (1 + Math.exp(-45 * volume + 5))
    return volume < 0.1 ? 0 : volume
  }
  
  stop(): void {
    if (this.currentSource) {
      this.currentSource.stop()
      this.currentSource = null
    }
  }
}
```

### Phase 2: VRM ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³åˆ¶å¾¡ã®ä¿®æ­£

```typescript
// vrm-animation.ts ã®ä¿®æ­£ç‰ˆ
export class VRMAnimationController {
  private audioLipSync: AudioLipSync | null = null
  
  constructor(vrm: VRM) {
    this.vrm = vrm
    this.audioLipSync = new AudioLipSync()
  }
  
  // âœ… ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹æœçš„ãªå£åˆ¶å¾¡
  private updateLipSync(): void {
    if (!this.audioLipSync || !this.isSpeaking) return
    
    const volume = this.audioLipSync.getVolume()
    const expressionManager = this.vrm.expressionManager
    
    if (expressionManager) {
      // âœ… 'aa' è¡¨æƒ…ã®ã¿åˆ¶å¾¡ï¼ˆä»–ã¯è§¦ã‚‰ãªã„ï¼‰
      const weight = volume * 0.5 // aituber-kit ã¨åŒã˜é‡ã¿
      expressionManager.setValue('aa', weight)
    }
  }
  
  update(deltaTime: number): void {
    // æ—¢å­˜ã®æ›´æ–°å‡¦ç†...
    this.updateLipSync() // â† æ¯ãƒ•ãƒ¬ãƒ¼ãƒ å‘¼ã³å‡ºã—
  }
  
  async speakWithAudio(audioBuffer: ArrayBuffer): Promise<void> {
    this.isSpeaking = true
    await this.audioLipSync.playWithLipSync(audioBuffer)
  }
  
  stopSpeaking(): void {
    this.isSpeaking = false
    this.audioLipSync?.stop()
    
    // å£ã‚’é–‰ã˜ã‚‹
    const expressionManager = this.vrm.expressionManager
    if (expressionManager) {
      expressionManager.setValue('aa', 0)
    }
  }
}
```

### Phase 3: çµ±åˆéŸ³å£°ã‚·ã‚¹ãƒ†ãƒ ã®ä¿®æ­£

```typescript
// unified-voice-synthesis.ts ã®ä¿®æ­£
export class UnifiedVoiceSynthesis {
  private audioLipSync: AudioLipSync | null = null
  
  constructor() {
    this.audioLipSync = new AudioLipSync()
  }
  
  private async speakWithVoicevox(
    text: string,
    emotion: Emotion,
    mode: 'asd' | 'nt',
    speaker: string | number,
    volume: number
  ): Promise<boolean> {
    try {
      this.callbacks.onStart?.()
      this.isPlaying = true

      // VOICEVOX ã§éŸ³å£°åˆæˆ
      const voiceOptions = createEmotionalVoiceOptions(text, emotion, mode, speaker)
      const result = await voicevoxClient.synthesizeWithTiming(voiceOptions)
      
      // âœ… AudioContext ã§å†ç”Ÿï¼ˆHTMLAudioElement ã§ã¯ãªã„ï¼‰
      await this.audioLipSync.playWithLipSync(result.audioBuffer)
      
      // âœ… VRM ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã« AudioLipSync ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æ¸¡ã™
      if (this.onAudioLipSyncReady) {
        this.onAudioLipSyncReady(this.audioLipSync)
      }
      
      return true
    } catch (error) {
      console.error('VOICEVOX synthesis failed:', error)
      return false
    }
  }
}
```

---

## ğŸš€ å®Ÿè£…å„ªå…ˆåº¦

### é«˜å„ªå…ˆåº¦ï¼ˆå³åº§ã«ä¿®æ­£ï¼‰
1. **AudioContext ãƒ™ãƒ¼ã‚¹ã®éŸ³å£°å†ç”Ÿã‚·ã‚¹ãƒ†ãƒ ** - HTMLAudioElement ã‚’å®Œå…¨ç½®æ›
2. **AnalyserNode æ¥ç¶š** - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°è§£æã®å®Ÿè£…
3. **è¡¨æƒ…ç«¶åˆã®é™¤å»** - å…¨è¡¨æƒ…ãƒªã‚»ãƒƒãƒˆå‡¦ç†ã®å‰Šé™¤

### ä¸­å„ªå…ˆåº¦ï¼ˆå‹•ä½œç¢ºèªå¾Œï¼‰
4. **aituber-kit æº–æ‹ ã®ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰éŸ³é‡å¤‰æ›** - è‡ªç„¶ãªå£ã®å‹•ã
5. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æ”¹å–„** - AudioContext ã®é©åˆ‡ãªç®¡ç†

### ä½å„ªå…ˆåº¦ï¼ˆæœ€é©åŒ–ãƒ•ã‚§ãƒ¼ã‚ºï¼‰
6. **éŸ³é‡èª¿æ•´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿** - æ„Ÿæƒ…ã«ã‚ˆã‚‹é‡ã¿èª¿æ•´
7. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–** - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ã®è»½é‡åŒ–

---

## ğŸ§ª æ¤œè¨¼æ–¹æ³•

### ãƒ†ã‚¹ãƒˆ 1: éŸ³å£°æ¥ç¶šç¢ºèª
```typescript
// AudioContext ã®æ¥ç¶šã‚’ç¢ºèª
const audioLipSync = new AudioLipSync()
console.log('AudioContext state:', audioLipSync.audioContext.state)
console.log('Analyser connected:', !!audioLipSync.analyser)
```

### ãƒ†ã‚¹ãƒˆ 2: éŸ³é‡å–å¾—ç¢ºèª  
```typescript
// ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³é‡ã‚’ç¢ºèª
setInterval(() => {
  const volume = audioLipSync.getVolume()
  console.log('Current volume:', volume)
}, 100)
```

### ãƒ†ã‚¹ãƒˆ 3: VRM è¡¨æƒ…åˆ¶å¾¡ç¢ºèª
```typescript
// æ‰‹å‹•ã§å£ã‚’å‹•ã‹ã—ã¦ç¢ºèª
const expressionManager = vrm.expressionManager
expressionManager.setValue('aa', 0.5) // å£ãŒé–‹ãã‹ç¢ºèª
```

---

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹çµæœ

### ä¿®æ­£å‰ï¼ˆç¾åœ¨ï¼‰
- âŒ å£ãŒå…¨ãå‹•ã‹ãªã„
- âŒ éŸ³å£°ã¨ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ãŒç„¡é–¢ä¿‚
- âŒ è¤‡é›‘ã§ç†è§£å›°é›£ãªã‚³ãƒ¼ãƒ‰

### ä¿®æ­£å¾Œï¼ˆæœŸå¾…å€¤ï¼‰
- âœ… éŸ³å£°ãƒœãƒªãƒ¥ãƒ¼ãƒ ã«å¿œã˜ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«å£ãŒå‹•ã
- âœ… éŸ³å£°ã¨å®Œå…¨ã«åŒæœŸã—ãŸãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯
- âœ… ã‚·ãƒ³ãƒ—ãƒ«ã§ä¿å®ˆã—ã‚„ã™ã„ã‚³ãƒ¼ãƒ‰
- âœ… aituber-kit ã¨åŒç­‰ã®å“è³ª

---

**çµè«–**: ç¾åœ¨ã®å®Ÿè£…ã¯æ ¹æœ¬çš„ã«é–“é•ã£ãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚aituber-kit ã®å®Ÿè¨¼æ¸ˆã¿ã® AudioContext + AnalyserNode æ–¹å¼ã«ç§»è¡Œã™ã‚‹ã“ã¨ã§ã€ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚

**æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**: AudioLipSync ã‚¯ãƒ©ã‚¹ã®å®Ÿè£…ã‹ã‚‰é–‹å§‹ã—ã€æ®µéšçš„ã«ç½®æ›ã—ã¦ã„ãã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚