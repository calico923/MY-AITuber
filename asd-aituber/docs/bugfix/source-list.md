# ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯é–¢é€£ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ä¸€è¦§

ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã®å•é¡Œèª¿æŸ»ã®ãŸã‚ã€é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ä¸€è¦§åŒ–ã—ã¦ã„ã¾ã™ã€‚

## 1. ChatPage (éŸ³å£°åˆæˆã¨ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã®èµ·ç‚¹)

**ãƒ•ã‚¡ã‚¤ãƒ«**: `apps/web/app/chat/page.tsx`

```typescript
'use client'

import { useRef, useEffect, useState } from 'react'
import ChatPanel from '@/components/ChatPanel'
import { ModeToggle } from '@/components/ModeToggle'
import { useChat } from '@/hooks/useChat'
import VRMViewer from '@/components/VRMViewer'
import type { VRMViewerRef } from '@/components/VRMViewer'
import type { Emotion } from '@asd-aituber/types'
import { useSimpleUnifiedVoice } from '@/hooks/useUnifiedVoiceSynthesis'

export default function ChatPage() {
  const { messages, isLoading, sendMessage, mode, changeMode } = useChat()
  const vrmViewerRef = useRef<VRMViewerRef>(null)
  const [currentEmotion, setCurrentEmotion] = useState<Emotion>('neutral')
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [mounted, setMounted] = useState(false)
  
  // éŸ³å£°åˆæˆæ©Ÿèƒ½ï¼ˆVOICEVOXçµ±åˆï¼‰
  const { speak: speakText, stop: stopSpeech, isSpeaking: isVoiceSpeaking, currentEngine } = useSimpleUnifiedVoice({
    preferredEngine: 'auto', // VOICEVOXãŒåˆ©ç”¨å¯èƒ½ãªã‚‰è‡ªå‹•é¸æŠ
    defaultMode: mode === 'asd' ? 'asd' : 'nt', // ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ã¨é€£å‹•
    volume: 0.8,
    callbacks: {} // ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ speak æ™‚ã«å€‹åˆ¥ã«è¨­å®š
  })
  
  // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã§ã®ã¿ãƒã‚¦ãƒ³ãƒˆ
  useEffect(() => {
    setMounted(true)
  }, [])

  // æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«åŸºã¥ã„ã¦æ„Ÿæƒ…ã‚’æ¨å®šã¨ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯
  useEffect(() => {
    if (messages.length === 0) return

    const lastMessage = messages[messages.length - 1]
    console.log('[ChatPage] ===== æ–°ã—ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ¤œå‡º =====')
    console.log('[ChatPage] Role:', lastMessage.role)
    console.log('[ChatPage] Content:', lastMessage.content)
    console.log('[ChatPage] Emotion:', lastMessage.emotion)
    
    if (lastMessage.role === 'assistant') {
      console.log('[ChatPage] ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãŸã‚ã€éŸ³å£°åˆæˆã‚’é–‹å§‹ã—ã¾ã™')
      // assistantãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ„Ÿæƒ…ã‚’åæ˜ 
      if (lastMessage.emotion) {
        setCurrentEmotion(lastMessage.emotion)
      }
      
      // éŸ³å£°åˆæˆã¨ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯
      const speakWithLipSync = async () => {
        console.log('[ChatPage] Starting speakWithLipSync')
        console.log('[ChatPage] Message content:', lastMessage.content)
        console.log('[ChatPage] Current engine:', currentEngine)
        
        // æ—¢å­˜ã®éŸ³å£°ã‚’åœæ­¢
        stopSpeech()
        
        // VRMãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚’é–‹å§‹
        if (vrmViewerRef.current) {
          console.log('[ChatPage] VRMViewer is available')
          setIsSpeaking(true)
          
          // å…ˆã«ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚’é–‹å§‹
          console.log('[ChatPage] Starting VRM lip sync first')
          if (vrmViewerRef.current.speakText) {
            vrmViewerRef.current.speakText(lastMessage.content, () => {
              console.log('[ChatPage] VRM lip sync completed')
            })
          }
          
          // éŸ³å£°åˆæˆã§è©±ã™ï¼ˆæ„Ÿæƒ…ã¨ãƒ¢ãƒ¼ãƒ‰ã‚’è€ƒæ…®ï¼‰
          console.log('[ChatPage] Starting voice synthesis with options:', {
            emotion: lastMessage.emotion || 'neutral',
            mode: mode === 'asd' ? 'asd' : 'nt'
          })
          
          await speakText(lastMessage.content, {
            emotion: lastMessage.emotion || 'neutral',
            mode: mode === 'asd' ? 'asd' : 'nt',
            callbacks: {
              onStart: () => {
                console.log('[ChatPage] Voice synthesis started callback triggered')
              },
              onEnd: () => {
                console.log('[ChatPage] Voice synthesis ended callback triggered')
                console.log('[ChatPage] ===== éŸ³å£°ä¼šè©±å®Œäº† =====')
                setIsSpeaking(false)
                // è©±ã—çµ‚ã‚ã£ãŸã‚‰3ç§’å¾Œã«è¡¨æƒ…ã‚’neutralã«æˆ»ã™
                setTimeout(() => {
                  if (vrmViewerRef.current) {
                    console.log('[ChatPage] Resetting emotion to neutral')
                    vrmViewerRef.current.setEmotion('neutral')
                    setCurrentEmotion('neutral')
                  }
                }, 3000)
              },
              onError: (error) => {
                console.error('[ChatPage] Speech synthesis error:', error)
                setIsSpeaking(false)
              }
            }
          })
        } else {
          console.log('[ChatPage] VRMViewer not available, playing voice only')
          // VRMViewerãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯éŸ³å£°ã®ã¿å†ç”Ÿ
          await speakText(lastMessage.content, {
            emotion: lastMessage.emotion || 'neutral',
            mode: mode === 'asd' ? 'asd' : 'nt',
            callbacks: {
              onStart: () => {
                console.log('[ChatPage] Voice-only: synthesis started')
                setIsSpeaking(true)
              },
              onEnd: () => {
                console.log('[ChatPage] Voice-only: synthesis ended')
                setIsSpeaking(false)
                setTimeout(() => setCurrentEmotion('neutral'), 3000)
              }
            }
          })
        }
      }
      
      // VRMViewerã®æº–å‚™ãŒã§ãã‚‹ã¾ã§å°‘ã—å¾…ã¤
      if (vrmViewerRef.current) {
        console.log('[ChatPage] VRMViewer is ready, executing speakWithLipSync immediately')
        speakWithLipSync()
      } else {
        console.log('[ChatPage] VRMViewer not ready, waiting 500ms')
        setTimeout(speakWithLipSync, 500)
      }
    } else {
      console.log('[ChatPage] Not an assistant message, skipping voice synthesis')
    }
  }, [messages])

  // ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çŠ¶æ…‹ã«åŸºã¥ã„ã¦è¡¨æƒ…ã‚’æ›´æ–°
  useEffect(() => {
    if (isLoading) {
      setCurrentEmotion('neutral')
      setIsSpeaking(false)
      // é€²è¡Œä¸­ã®éŸ³å£°åˆæˆã‚’åœæ­¢
      stopSpeech()
    }
  }, [isLoading, stopSpeech])

  // ãƒã‚¦ãƒ³ãƒˆå‰ã¯ä½•ã‚‚è¡¨ç¤ºã—ãªã„
  if (!mounted) {
    return null
  }

  return (
    <div className="min-h-screen flex">
      {/* å·¦å´: VRMViewer */}
      <div className="flex-1 relative">
        <VRMViewer
          ref={vrmViewerRef}
          modelUrl="/models/sample.vrm"
          emotion={currentEmotion}
          isSpeaking={isSpeaking}
        />
        
        {/* ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆãƒœã‚¿ãƒ³ */}
        <div className="absolute top-4 right-4">
          <ModeToggle 
            currentMode={mode} 
            onModeChange={changeMode}
          />
        </div>
      </div>
      
      {/* å³å´: ãƒãƒ£ãƒƒãƒˆãƒ‘ãƒãƒ« */}
      <div className="w-96 border-l border-border">
        <ChatPanel
          messages={messages}
          isLoading={isLoading}
          onSendMessage={sendMessage}
          currentMode={mode}
        />
      </div>
    </div>
  )
}
```

## 2. VRMViewer (VRMãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¤ºã¨ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³åˆ¶å¾¡)

**ãƒ•ã‚¡ã‚¤ãƒ«**: `components/VRMViewer.tsx`

```typescript
'use client'

import { useRef, useEffect, useState, forwardRef, useImperativeHandle } from 'react'
import * as THREE from 'three'
import { GLTFLoader } from 'three/examples/jsm/loaders/GLTFLoader.js'
import { VRM, VRMLoaderPlugin } from '@pixiv/three-vrm'
import { OrbitControls } from 'three/examples/jsm/controls/OrbitControls.js'
import type { Emotion } from '@asd-aituber/types'
import { VRMAnimationController } from '@/lib/vrm-animation'

export interface VRMViewerRef {
  setEmotion: (emotion: Emotion) => void
  setSpeaking: (intensity: number) => void
  speakText: (text: string, onComplete?: () => void) => void
  stopSpeaking: () => void
  getAvailableExpressions: () => string[]
}

interface VRMViewerProps {
  modelUrl?: string
  emotion?: Emotion
  isSpeaking?: boolean
}

const VRMViewer = forwardRef<VRMViewerRef, VRMViewerProps>(
  ({ modelUrl, emotion = 'neutral', isSpeaking = false }, ref) => {
    const mountRef = useRef<HTMLDivElement>(null)
    const sceneRef = useRef<THREE.Scene | null>(null)
    const rendererRef = useRef<THREE.WebGLRenderer | null>(null)
    const vrmRef = useRef<VRM | null>(null)
    const animationControllerRef = useRef<VRMAnimationController | null>(null)
    const controlsRef = useRef<OrbitControls | null>(null)
    const clockRef = useRef(new THREE.Clock())
    
    const [isLoading, setIsLoading] = useState(false)
    const [error, setError] = useState<string | null>(null)
    const [availableExpressions, setAvailableExpressions] = useState<string[]>([])

    // Imperativeãƒãƒ³ãƒ‰ãƒ«ã®è¨­å®š
    useImperativeHandle(ref, () => {
      return {
        setEmotion: (emotion: Emotion) => {
          if (animationControllerRef.current) {
            animationControllerRef.current.setEmotion(emotion)
          }
        },
        setSpeaking: (intensity: number) => {
          if (animationControllerRef.current) {
            animationControllerRef.current.setSpeaking(intensity)
          }
        },
        speakText: (text: string, onComplete?: () => void) => {
          console.log('VRMViewer.speakText called with:', text)
          if (animationControllerRef.current) {
            console.log('Animation controller available, calling speakText')
            animationControllerRef.current.speakText(text, onComplete)
          } else {
            console.warn('Animation controller not available for speakText')
          }
        },
        stopSpeaking: () => {
          if (animationControllerRef.current) {
            animationControllerRef.current.stopSpeaking()
          }
        },
        getAvailableExpressions: () => availableExpressions
      }
    }, [availableExpressions])

    // æ„Ÿæƒ…å¤‰åŒ–ã®ç›£è¦–
    useEffect(() => {
      if (animationControllerRef.current) {
        animationControllerRef.current.setEmotion(emotion)
      }
    }, [emotion])

    // è©±ã—ã¦ã„ã‚‹çŠ¶æ…‹ã®ç›£è¦–
    useEffect(() => {
      if (animationControllerRef.current) {
        animationControllerRef.current.setSpeaking(isSpeaking ? 0.7 : 0)
      }
    }, [isSpeaking])

  useEffect(() => {
    if (!mountRef.current) return

    const container = mountRef.current
    const rect = container.getBoundingClientRect()
    const width = rect.width || 640
    const height = rect.height || 480

    // Scene setup
    const scene = new THREE.Scene()
    sceneRef.current = scene
    scene.background = new THREE.Color(0x87ceeb)

    // Camera setup - å…¨èº«ãŒè¦‹ãˆã‚‹ã‚ˆã†ã«èª¿æ•´
    const camera = new THREE.PerspectiveCamera(30, width / height, 0.1, 20)
    camera.position.set(0, 1.4, 3)

    // Renderer setup
    const renderer = new THREE.WebGLRenderer({ antialias: true })
    rendererRef.current = renderer
    renderer.setSize(width, height)
    renderer.setPixelRatio(window.devicePixelRatio)
    renderer.outputColorSpace = THREE.SRGBColorSpace
    container.appendChild(renderer.domElement)

    // Controls
    const controls = new OrbitControls(camera, renderer.domElement)
    controlsRef.current = controls
    controls.enableDamping = true
    controls.target.set(0, 1.4, 0)
    controls.update()

    // Lighting
    const ambientLight = new THREE.AmbientLight(0xffffff, 0.5)
    scene.add(ambientLight)
    const directionalLight = new THREE.DirectionalLight(0xffffff, 0.5)
    directionalLight.position.set(1, 1, 1)
    scene.add(directionalLight)

    // Load VRM model
    if (modelUrl) {
      setIsLoading(true)
      setError(null)
      
      const loader = new GLTFLoader()
      loader.register((parser) => new VRMLoaderPlugin(parser))
      
      loader.load(
        modelUrl,
        (gltf) => {
          const vrm = gltf.userData.vrm as VRM
          if (vrm) {
            scene.add(vrm.scene)
            vrmRef.current = vrm
            
            // Initialize animation controller
            Promise.resolve().then(() => {
              animationControllerRef.current = new VRMAnimationController(vrm)
              
              // åˆ©ç”¨å¯èƒ½ãªè¡¨æƒ…ã‚’å–å¾—
              const expressions = animationControllerRef.current.getAvailableExpressions()
              setAvailableExpressions(expressions)
              console.log('Available VRM expressions:', expressions)
              console.log('VRM animation controller initialized successfully')
              console.log('VRM initial pose set')
            })
            
            setIsLoading(false)
          } else {
            setError('Failed to load VRM from GLTF')
            setIsLoading(false)
          }
        },
        (progress) => {
          console.log('Loading progress:', (progress.loaded / progress.total) * 100, '%')
        },
        (error) => {
          console.error('Error loading VRM:', error)
          setError(`Error loading VRM: ${error instanceof Error ? error.message : 'Unknown error'}`)
          setIsLoading(false)
        }
      )
    }

    // Animation loop
    const animate = () => {
      requestAnimationFrame(animate)
      
      const deltaTime = clockRef.current.getDelta()
      
      // ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã®æ›´æ–°
      if (controlsRef.current) {
        controlsRef.current.update()
      }
      
      // ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã®æ›´æ–°ï¼ˆVRMæ›´æ–°ã®å‰ã«å®Ÿè¡Œï¼‰
      if (animationControllerRef.current) {
        animationControllerRef.current.update(deltaTime)
      }
      
      // VRMã®åŸºæœ¬æ›´æ–°ï¼ˆexpressionManagerã®å¤‰æ›´ã‚’åæ˜ ï¼‰
      if (vrmRef.current) {
        // è¡¨æƒ…ã®ç¾åœ¨å€¤ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆä¸€å®šé–“éš”ã§ï¼‰
        if (Math.floor(performance.now() / 1000) !== Math.floor((performance.now() - 16) / 1000)) {
          const expressionManager = vrmRef.current.expressionManager
          if (expressionManager) {
            try {
              const aaValue = expressionManager.getValue('aa')
              const happyValue = expressionManager.getValue('happy')
              if (aaValue > 0 || happyValue > 0) {
                console.log(`[VRMViewer] Expression values: aa=${aaValue.toFixed(3)}, happy=${happyValue.toFixed(3)}`)
              }
            } catch {
              // Silent fail for non-existent expressions
            }
          }
        }
        
        vrmRef.current.update(deltaTime)
      }

      renderer.render(scene, camera)
    }
    animate()

    // Handle window resize
    const handleResize = () => {
      const rect = container.getBoundingClientRect()
      const newWidth = rect.width || 640
      const newHeight = rect.height || 480

      camera.aspect = newWidth / newHeight
      camera.updateProjectionMatrix()
      renderer.setSize(newWidth, newHeight)
    }

    window.addEventListener('resize', handleResize)

    return () => {
      window.removeEventListener('resize', handleResize)
      
      if (container && renderer.domElement) {
        container.removeChild(renderer.domElement)
      }
      renderer.dispose()
      
      // VRM cleanup
      if (vrmRef.current) {
        vrmRef.current.dispose()
      }
    }
  }, [modelUrl])

  return (
    <div className="relative w-full h-full">
      <div ref={mountRef} className="w-full h-full" />
      
      {isLoading && (
        <div className="absolute inset-0 flex items-center justify-center bg-background bg-opacity-75">
          <div className="text-center">
            <div className="animate-spin rounded-full h-16 w-16 border-b-2 border-primary mx-auto mb-4"></div>
            <div className="text-lg">Loading VRM model...</div>
          </div>
        )}
        {error && (
          <div className="absolute inset-0 flex items-center justify-center bg-red-100 bg-opacity-75">
            <div className="text-red-600">{error}</div>
          </div>
        )}
        {!modelUrl && !isLoading && (
          <div className="absolute bottom-4 left-4 bg-black bg-opacity-50 text-white p-2 rounded">
            Test cube (no VRM model loaded)
          </div>
        )}
        
        {/* ã‚«ãƒ¡ãƒ©æ“ä½œèª¬æ˜ */}
        <div className="absolute top-4 left-4 bg-black bg-opacity-50 text-white p-2 rounded text-xs">
          <div>ğŸ–±ï¸ ãƒ‰ãƒ©ãƒƒã‚°: å›è»¢</div>
          <div>ğŸ” ãƒ›ã‚¤ãƒ¼ãƒ«: ã‚ºãƒ¼ãƒ </div>
          <div>âŒ¨ï¸ å³ã‚¯ãƒªãƒƒã‚¯+ãƒ‰ãƒ©ãƒƒã‚°: ãƒ‘ãƒ³</div>
        </div>

        {availableExpressions.length > 0 && (
          <div className="absolute bottom-4 right-4 bg-black bg-opacity-50 text-white p-2 rounded text-xs">
            <div>Available expressions: {availableExpressions.length}</div>
            <div>Current emotion: {emotion}</div>
            {isSpeaking && <div>ğŸ—£ï¸ Speaking</div>}
          </div>
        )}
      </div>
    )
  }
)

VRMViewer.displayName = 'VRMViewer'

export default VRMViewer
```

## 3. VRMAnimationController (VRMã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³åˆ¶å¾¡)

**ãƒ•ã‚¡ã‚¤ãƒ«**: `lib/vrm-animation.ts`

```typescript
import * as THREE from 'three'
import type { VRM } from '@pixiv/three-vrm'
import type { Emotion } from '@asd-aituber/types'
import { LipSync } from './lip-sync'

export class VRMAnimationController {
  private vrm: VRM
  private clock: number = 0
  private lipSync: LipSync
  private isSpeaking: boolean = false
  private currentEmotion: Emotion = 'neutral'
  
  // ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
  private blinkInterval: number = 3000 // 3ç§’é–“éš”
  private lastBlinkTime: number = 0
  private breathingSpeed: number = 0.001
  private breathingIntensity: number = 0.02
  private armSwaySpeed: number = 0.0008
  private armSwayIntensity: number = 0.05
  private legShiftSpeed: number = 0.0005
  private legShiftIntensity: number = 0.03

  constructor(vrm: VRM) {
    this.vrm = vrm
    this.lipSync = new LipSync()
    this.setInitialPose()
  }

  /**
   * åˆæœŸãƒãƒ¼ã‚ºã®è¨­å®šï¼ˆè‡ªç„¶ãªç«‹ã¡ãƒãƒ¼ã‚ºï¼‰
   */
  private setInitialPose(): void {
    if (!this.vrm.humanoid) return

    try {
      // è…•ã®è‡ªç„¶ãªä½ç½®
      const leftUpperArm = this.vrm.humanoid.getNormalizedBoneNode('leftUpperArm')
      const rightUpperArm = this.vrm.humanoid.getNormalizedBoneNode('rightUpperArm')
      const leftLowerArm = this.vrm.humanoid.getNormalizedBoneNode('leftLowerArm')
      const rightLowerArm = this.vrm.humanoid.getNormalizedBoneNode('rightLowerArm')

      if (leftUpperArm) {
        leftUpperArm.rotation.z = Math.PI * 0.15  // å·¦è…•ã‚’å°‘ã—é–‹ã
        leftUpperArm.rotation.x = Math.PI * 0.05  // å°‘ã—å‰ã«
      }
      if (rightUpperArm) {
        rightUpperArm.rotation.z = -Math.PI * 0.15 // å³è…•ã‚’å°‘ã—é–‹ã
        rightUpperArm.rotation.x = Math.PI * 0.05  // å°‘ã—å‰ã«
      }
      if (leftLowerArm) {
        leftLowerArm.rotation.y = Math.PI * 0.1   // è‚˜ã‚’æ›²ã’ã‚‹
        leftLowerArm.rotation.z = Math.PI * 0.05  // å†…å´ã«å‘ã‘ã‚‹
      }
      if (rightLowerArm) {
        rightLowerArm.rotation.y = -Math.PI * 0.1  // è‚˜ã‚’æ›²ã’ã‚‹
        rightLowerArm.rotation.z = -Math.PI * 0.05 // å†…å´ã«å‘ã‘ã‚‹
      }

      // é¦–ã¨é ­ã®è‡ªç„¶ãªä½ç½®
      const neck = this.vrm.humanoid.getNormalizedBoneNode('neck')
      const head = this.vrm.humanoid.getNormalizedBoneNode('head')
      
      if (neck) {
        neck.rotation.x = Math.PI * 0.02 // é¦–ã‚’å°‘ã—å‰ã«å‚¾ã‘ã‚‹
      }
      if (head) {
        head.rotation.x = Math.PI * 0.01 // é ­ã‚’å°‘ã—ä¸‹ã«å‘ã‘ã‚‹
      }

      // è„Šæ¤ã®è‡ªç„¶ãªã‚«ãƒ¼ãƒ–
      const spine = this.vrm.humanoid.getNormalizedBoneNode('spine')
      if (spine) {
        spine.rotation.x = -Math.PI * 0.01 // èƒŒç­‹ã‚’å°‘ã—ä¼¸ã°ã™
      }

      console.log('VRM initial pose set successfully')
    } catch (error) {
      console.warn('Failed to set initial pose:', error)
    }
  }

  /**
   * ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«å‘¼ã°ã‚Œã‚‹æ›´æ–°å‡¦ç†
   * @param deltaTime - å‰ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ã®çµŒéæ™‚é–“
   */
  update(deltaTime: number): void {
    this.clock += deltaTime * 1000 // ãƒŸãƒªç§’ã«å¤‰æ›

    // åŸºæœ¬çš„ãªã‚¢ã‚¤ãƒ‰ãƒ«ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
    this.updateIdleAnimations()

    // è©±ã—ã¦ã„ã‚‹æ™‚ã®è¿½åŠ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
    if (this.isSpeaking) {
      this.updateSpeakingAnimations()
    }

    // æ„Ÿæƒ…ã«å¿œã˜ãŸè¡¨æƒ…ã®é©ç”¨
    this.applyEmotionExpression(this.currentEmotion)
  }

  /**
   * ã‚¢ã‚¤ãƒ‰ãƒ«æ™‚ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
   */
  private updateIdleAnimations(): void {
    this.updateBlinking()
    this.updateBreathing()
    this.updateArmSway()
    this.updateLegShift()
  }

  /**
   * è©±ã—ã¦ã„ã‚‹æ™‚ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
   */
  private updateSpeakingAnimations(): void {
    // ã‚ˆã‚Šæ´»ç™ºãªã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚’é©ç”¨
    const speakingIntensity = 0.7
    this.updateSpeakingGestures(speakingIntensity)
  }

  /**
   * ã¾ã°ãŸãã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
   */
  private updateBlinking(): void {
    const expressionManager = this.vrm.expressionManager
    if (!expressionManager) return

    const currentTime = this.clock
    if (currentTime - this.lastBlinkTime > this.blinkInterval) {
      try {
        // ã¾ã°ãŸãå®Ÿè¡Œ
        if (expressionManager.getExpressionTrackName('blink')) {
          expressionManager.setValue('blink', 1.0)
          
          // 200mså¾Œã«ã¾ã°ãŸãã‚’æˆ»ã™
          setTimeout(() => {
            if (expressionManager.getExpressionTrackName('blink')) {
              expressionManager.setValue('blink', 0.0)
            }
          }, 200)
        }
        
        this.lastBlinkTime = currentTime
        // æ¬¡ã®ã¾ã°ãŸãã¾ã§ã®æ™‚é–“ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«èª¿æ•´
        this.blinkInterval = 2000 + Math.random() * 4000 // 2-6ç§’
      } catch (error) {
        // ã¾ã°ãŸãè¡¨æƒ…ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
      }
    }
  }

  /**
   * å‘¼å¸ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆèƒ¸ã®ä¸Šä¸‹å‹•ï¼‰
   */
  private updateBreathing(): void {
    if (!this.vrm.humanoid) return

    try {
      const chest = this.vrm.humanoid.getNormalizedBoneNode('chest')
      if (chest) {
        const breathingOffset = Math.sin(this.clock * this.breathingSpeed) * this.breathingIntensity
        chest.rotation.x = breathingOffset
      }
    } catch (error) {
      // Breathing animation failed, skip silently
    }
  }

  /**
   * è…•ã®å¾®ç´°ãªå‹•ãï¼ˆå¾…æ©Ÿæ™‚ã®è‡ªç„¶ãªæºã‚Œï¼‰
   */
  private updateArmSway(): void {
    if (!this.vrm.humanoid) return

    // å·¦è…•ã®å‹•ã
    this.updateArmSwayForSide('leftUpperArm', 'leftLowerArm', 1)
    // å³è…•ã®å‹•ã
    this.updateArmSwayForSide('rightUpperArm', 'rightLowerArm', -1)
  }

  /**
   * ç‰‡è…•ã®æºã‚Œã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³
   * @param upperArmName - ä¸Šè…•ã®ãƒœãƒ¼ãƒ³å
   * @param lowerArmName - å‰è…•ã®ãƒœãƒ¼ãƒ³å  
   * @param side - å·¦å³ã®å‘ãï¼ˆ1: å·¦, -1: å³ï¼‰
   */
  private updateArmSwayForSide(upperArmName: string, lowerArmName: string, side: number): void {
    if (!this.vrm.humanoid) return

    try {
      const upperArm = this.vrm.humanoid.getNormalizedBoneNode(upperArmName as any)
      const lowerArm = this.vrm.humanoid.getNormalizedBoneNode(lowerArmName as any)

      if (upperArm) {
        // åŸºæœ¬ãƒãƒ¼ã‚ºã‚’ç¶­æŒã—ã¤ã¤å¾®ç´°ãªå‹•ã
        const baseRotationZ = Math.PI * 0.15 * side  // åˆæœŸãƒãƒ¼ã‚ºã¨åŒã˜é–‹ãå…·åˆ
        const baseRotationX = Math.PI * 0.05         // åˆæœŸãƒãƒ¼ã‚ºã¨åŒã˜å‰å‚¾
        const armSway = Math.sin(this.clock * this.armSwaySpeed) * this.armSwayIntensity
        
        upperArm.rotation.z = baseRotationZ + armSway * side * 0.3
        upperArm.rotation.x = baseRotationX + armSway * 0.2
      }

      if (lowerArm) {
        // åŸºæœ¬ã®æ›²ã’ãŸå‰è…•ã®ä½ç½®ã‚’ç¶­æŒã—ã¤ã¤å¾®ç´°ãªå‹•ã
        const baseRotationY = Math.PI * 0.1 * side   // åˆæœŸãƒãƒ¼ã‚ºã¨åŒã˜æ›²ã’ãŸä½ç½®
        const baseRotationZ = Math.PI * 0.05 * side  // åˆæœŸãƒãƒ¼ã‚ºã¨åŒã˜å†…å´å‘ã
        const forearmBend = Math.sin(this.clock * this.armSwaySpeed * 1.3) * this.armSwayIntensity * 0.2
        
        lowerArm.rotation.y = baseRotationY + forearmBend * side * 0.5
        lowerArm.rotation.z = baseRotationZ
      }
    } catch (error) {
      // Arm animation failed, skip silently
    }
  }

  /**
   * è¶³ã®å¾…æ©Ÿã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé‡å¿ƒç§»å‹•ï¼‰
   * @param leftLegName - å·¦è¶³ã®ãƒœãƒ¼ãƒ³å
   * @param rightLegName - å³è¶³ã®ãƒœãƒ¼ãƒ³å
   */
  private updateLegIdle(leftLegName: string, rightLegName: string): void {
    if (!this.vrm.humanoid) return

    try {
      const leftLeg = this.vrm.humanoid.getNormalizedBoneNode(leftLegName as any)
      const rightLeg = this.vrm.humanoid.getNormalizedBoneNode(rightLegName as any)

      // é‡å¿ƒç§»å‹•ï¼ˆå·¦å³ã®è¶³ã«äº¤äº’ã«ä½“é‡ã‚’ã‹ã‘ã‚‹ï¼‰
      const weightShift = Math.sin(this.clock * this.legShiftSpeed) * this.legShiftIntensity

      if (leftLeg) {
        leftLeg.rotation.z = weightShift * 0.5
      }

      if (rightLeg) {
        rightLeg.rotation.z = -weightShift * 0.5
      }
    } catch (error) {
      // Leg animation failed, skip silently
    }
  }

  /**
   * æ„Ÿæƒ…ã‚’è¨­å®š
   * @param emotion - è¨­å®šã™ã‚‹æ„Ÿæƒ…
   */
  setEmotion(emotion: Emotion): void {
    this.currentEmotion = emotion
  }

  /**
   * æ„Ÿæƒ…ã«å¿œã˜ãŸè¡¨æƒ…ã‚’é©ç”¨
   * @param emotion - é©ç”¨ã™ã‚‹æ„Ÿæƒ…
   */
  private applyEmotionExpression(emotion: Emotion): void {
    const expressionManager = this.vrm.expressionManager
    if (!expressionManager) return

    // å…¨ã¦ã®è¡¨æƒ…ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆå£ãƒ‘ã‚¯ç”¨ã®æ¯éŸ³ã¯é™¤ãï¼‰
    const expressions = ['happy', 'sad', 'angry', 'surprised', 'relaxed', 'neutral']
    expressions.forEach(expr => {
      try {
        if (expressionManager.getExpressionTrackName(expr)) {
          expressionManager.setValue(expr, 0)
        }
      } catch (error) {
        // Expression not available, skip silently
      }
    })

    // æ„Ÿæƒ…ã«å¿œã˜ãŸè¡¨æƒ…ã‚’é©ç”¨
    try {
      let targetExpression = ''
      let intensity = 0.7

      switch (emotion) {
        case 'joy':
        case 'happy':
          targetExpression = 'happy'
          intensity = 0.8
          break
        case 'sadness':
        case 'sad':
          targetExpression = 'sad'
          intensity = 0.7
          break
        case 'anger':
        case 'angry':
          targetExpression = 'angry'
          intensity = 0.6
          break
        case 'surprise':
        case 'surprised':
          targetExpression = 'surprised'
          intensity = 0.9
          break
        case 'fear':
          targetExpression = 'sad' // fearãŒãªã„å ´åˆã¯sadã§ä»£ç”¨
          intensity = 0.5
          break
        case 'disgust':
          targetExpression = 'angry' // disgustãŒãªã„å ´åˆã¯angryã§ä»£ç”¨
          intensity = 0.4
          break
        case 'neutral':
        default:
          targetExpression = 'neutral'
          intensity = 1.0
          break
      }

      if (targetExpression && expressionManager.getExpressionTrackName(targetExpression)) {
        expressionManager.setValue(targetExpression, intensity)
      }
    } catch (error) {
      // Expression application failed, skip silently
    }
  }

  /**
   * è©±ã™å¼·åº¦ã‚’è¨­å®š
   * @param intensity - è©±ã™å¼·åº¦ï¼ˆ0-1ï¼‰
   */
  setSpeaking(intensity: number): void {
    this.isSpeaking = intensity > 0

    // è©±ã—ã¦ã„ã‚‹æ™‚ã®å…¨èº«ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼
    this.updateSpeakingGestures(intensity)
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã‚’è©±ã™ï¼ˆãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ä»˜ãï¼‰
   * @param text - è©±ã™ãƒ†ã‚­ã‚¹ãƒˆ
   * @param onComplete - å®Œäº†æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
   */
  speakText(text: string, onComplete?: () => void): void {
    console.log('[VRMAnimationController] speakText called - using simple lip sync')
    this.isSpeaking = true
    
    // aituber-kitæ–¹å¼ï¼šã‚·ãƒ³ãƒ—ãƒ«ãªãƒœãƒªãƒ¥ãƒ¼ãƒ åŸºç›¤ã®ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯
    this.startSimpleLipSync(text.length, onComplete)
  }
  
  /**
   * ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ï¼ˆaituber-kitæ–¹å¼ï¼‰
   */
  private startSimpleLipSync(textLength: number, onComplete?: () => void): void {
    // æ–‡å­—æ•°ã«åŸºã¥ã„ã¦è©±ã™æ™‚é–“ã‚’è¨ˆç®—
    const duration = Math.max(1000, textLength * 100) // æ–‡å­—ã‚ãŸã‚Š100msã€æœ€ä½1ç§’
    const startTime = performance.now()
    
    console.log(`[VRMAnimationController] Starting simple lip sync for ${duration}ms`)
    
    const animate = () => {
      const elapsed = performance.now() - startTime
      const progress = elapsed / duration
      
      if (progress < 1 && this.isSpeaking) {
        // ã‚µã‚¤ãƒ³æ³¢ã§ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’å¤‰å‹•
        const volume = (Math.sin(elapsed * 0.01) + 1) * 0.4 + 0.1 // 0.1-0.9ã®ç¯„å›²
        
        // ç›´æ¥'aa'è¡¨æƒ…ã‚’è¨­å®š
        const expressionManager = this.vrm.expressionManager
        if (expressionManager) {
          try {
            if (expressionManager.getExpressionTrackName('aa')) {
              expressionManager.setValue('aa', volume)
              console.log(`[VRMAnimationController] Simple lip sync: setting 'aa' to ${volume.toFixed(3)}`)
            } else if (expressionManager.getExpressionTrackName('happy')) {
              expressionManager.setValue('happy', volume * 0.3)
              console.log(`[VRMAnimationController] Simple lip sync: setting 'happy' to ${(volume * 0.3).toFixed(3)}`)
            }
          } catch (error) {
            console.error('[VRMAnimationController] Error in simple lip sync:', error)
          }
        }
        
        requestAnimationFrame(animate)
      } else {
        // ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯çµ‚äº†
        console.log('[VRMAnimationController] Simple lip sync completed')
        this.isSpeaking = false
        this.setMouthShape('silence', 0)
        if (onComplete) {
          onComplete()
        }
      }
    }
    
    animate()
  }

  /**
   * å£ã®å½¢ã‚’è¨­å®š
   * @param phoneme - éŸ³éŸ»ï¼ˆa, i, u, e, o, silenceï¼‰
   * @param intensity - å¼·åº¦ï¼ˆ0-1ï¼‰
   */
  private setMouthShape(phoneme: string, intensity: number): void {
    console.log(`[VRMAnimationController] setMouthShape: ${phoneme}, intensity: ${intensity.toFixed(2)}`)

    const expressionManager = this.vrm.expressionManager
    if (!expressionManager) {
      console.warn('[VRMAnimationController] No expression manager available')
      return
    }

    console.log(`[VRMAnimationController] setMouthShape: ${phoneme}, intensity: ${intensity.toFixed(2)}`)

    // åˆ©ç”¨å¯èƒ½ãªè¡¨æƒ…ã‚’ç¢ºèªï¼ˆåˆå›ã®ã¿ãƒ­ã‚°å‡ºåŠ›ï¼‰
    if (this.vowelExpressionInfo === undefined) {
      this.vowelExpressionInfo = this.checkVowelExpressions()
      console.log('[VRMAnimationController] Mouth expressions:', this.vowelExpressionInfo)
    }
    
    if (this.vowelExpressionInfo.type !== 'none') {
      // å£ã®è¡¨æƒ…ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆ
      console.log(`[VRMAnimationController] Using ${this.vowelExpressionInfo.type} expressions`)
      this.setMouthExpression(phoneme, intensity)
    } else {
      // å£ã®è¡¨æƒ…ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ä»£æ›¿å‡¦ç†
      console.log('[VRMAnimationController] Using alternative mouth shape')
      this.setAlternativeMouthShape(phoneme, intensity)
    }
  }
  
  private vowelExpressionInfo?: { type: 'standard' | 'legacy' | 'none', available: string[] }
  private alternativeLogShown?: boolean
  
  /**
   * å£ã®è¡¨æƒ…ã®å­˜åœ¨ã‚’ç¢ºèªï¼ˆVRMæ¨™æº–è¡¨æƒ…å„ªå…ˆï¼‰
   */
  private checkVowelExpressions(): { type: 'standard' | 'legacy' | 'none', available: string[] } {
    const expressionManager = this.vrm.expressionManager
    if (!expressionManager) return { type: 'none', available: [] }
    
    // å…¨ã¦ã®åˆ©ç”¨å¯èƒ½ãªè¡¨æƒ…ã‚’è©³ç´°ã«ãƒ­ã‚°å‡ºåŠ›
    this.debugAllExpressions()
    
    // VRMæ¨™æº–è¡¨æƒ…ã‚’ãƒã‚§ãƒƒã‚¯
    const standardMouthExpressions = ['aa', 'ih', 'ou', 'ee', 'oh']
    const availableStandard = standardMouthExpressions.filter(expr => {
      try {
        const trackName = expressionManager.getExpressionTrackName(expr)
        if (trackName) {
          console.log(`[VRMAnimationController] Found standard expression: ${expr} -> ${trackName}`)
          return true
        }
        return false
      } catch {
        return false
      }
    })
    
    if (availableStandard.length > 0) {
      return { type: 'standard', available: availableStandard }
    }
    
    // æ—§å½¢å¼ã®æ¯éŸ³è¡¨æƒ…ã‚’ãƒã‚§ãƒƒã‚¯
    const legacyVowels = ['a', 'i', 'u', 'e', 'o']
    const availableLegacy = legacyVowels.filter(vowel => {
      try {
        const trackName = expressionManager.getExpressionTrackName(vowel)
        if (trackName) {
          console.log(`[VRMAnimationController] Found legacy expression: ${vowel} -> ${trackName}`)
          return true
        }
        return false
      } catch {
        return false
      }
    })
    
    if (availableLegacy.length > 0) {
      return { type: 'legacy', available: availableLegacy }
    }
    
    return { type: 'none', available: [] }
  }
  
  /**
   * ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šå…¨ã¦ã®åˆ©ç”¨å¯èƒ½ãªè¡¨æƒ…ã‚’å‡ºåŠ›
   */
  private debugAllExpressions(): void {
    const expressionManager = this.vrm.expressionManager
    if (!expressionManager) return

    console.log('[VRMAnimationController] === Debugging all expressions ===')
    
    try {
      // expressionMapã‹ã‚‰å…¨ã¦ã®è¡¨æƒ…ã‚’å–å¾—
      if (expressionManager.expressionMap) {
        const allExpressions = Object.keys(expressionManager.expressionMap)
        console.log('[VRMAnimationController] All expressions in expressionMap:', allExpressions)
        
        // å„è¡¨æƒ…ã®è©³ç´°æƒ…å ±
        allExpressions.forEach(expr => {
          try {
            const trackName = expressionManager.getExpressionTrackName(expr)
            const currentValue = expressionManager.getValue(expr)
            console.log(`[VRMAnimationController] Expression "${expr}": track="${trackName}", value=${currentValue}`)
          } catch (error) {
            console.log(`[VRMAnimationController] Expression "${expr}": Error accessing - ${error}`)
          }
        })
      } else {
        console.log('[VRMAnimationController] No expressionMap available')
      }
    } catch (error) {
      console.error('[VRMAnimationController] Error debugging expressions:', error)
    }
    
    console.log('[VRMAnimationController] === End expression debug ===')
  }
  
  /**
   * å£ã®è¡¨æƒ…ã‚’è¨­å®šï¼ˆVRMæ¨™æº–ã¾ãŸã¯ãƒ¬ã‚¬ã‚·ãƒ¼å½¢å¼ï¼‰
   */
  private setMouthExpression(phoneme: string, intensity: number): void {
    const expressionManager = this.vrm.expressionManager!
    const info = this.vowelExpressionInfo!
    
    if (info.type === 'standard') {
      this.setStandardMouthExpression(phoneme, intensity)
    } else if (info.type === 'legacy') {
      this.setLegacyVowelExpression(phoneme, intensity)
    }
  }
  
  /**
   * VRMæ¨™æº–ã®å£è¡¨æƒ…ã‚’è¨­å®šï¼ˆaituber-kitæ–¹å¼ï¼‰
   */
  private setStandardMouthExpression(phoneme: string, intensity: number): void {
    const expressionManager = this.vrm.expressionManager!
    const available = this.vowelExpressionInfo!.available
    
    // å…¨ã¦ã®å£è¡¨æƒ…ã‚’ãƒªã‚»ãƒƒãƒˆ
    const allMouthExpressions = ['aa', 'ih', 'ou', 'ee', 'oh']
    allMouthExpressions.forEach(expr => {
      if (available.includes(expr)) {
        try {
          expressionManager.setValue(expr, 0)
        } catch (error) {
          // Skip silently
        }
      }
    })

    // aituber-kitæ–¹å¼ï¼šä¸»ã«'aa'è¡¨æƒ…ã‚’ä½¿ç”¨
    if (phoneme !== 'silence' && available.includes('aa')) {
      try {
        console.log(`[VRMAnimationController] Setting 'aa' expression to ${intensity}`)
        expressionManager.setValue('aa', intensity)
        
        // è¨­å®šå¾Œã®å€¤ã‚’ç¢ºèª
        const currentValue = expressionManager.getValue('aa')
        console.log(`[VRMAnimationController] 'aa' expression value after setting: ${currentValue}`)
        
        if (!this.alternativeLogShown) {
          console.log('[VRMAnimationController] Using VRM standard \'aa\' expression for lip sync')
          this.alternativeLogShown = true
        }
      } catch (error) {
        console.error(`[VRMAnimationController] Error setting 'aa' expression:`, error)
      }
    } else {
      console.log(`[VRMAnimationController] Skipping 'aa' - phoneme: ${phoneme}, available: ${available.join(', ')}`)
    }
  }
  
  /**
   * ãƒ¬ã‚¬ã‚·ãƒ¼æ¯éŸ³è¡¨æƒ…ã‚’è¨­å®š
   */
  private setLegacyVowelExpression(phoneme: string, intensity: number): void {
    const expressionManager = this.vrm.expressionManager!
    const available = this.vowelExpressionInfo!.available
    
    // å…¨ã¦ã®æ¯éŸ³ã‚’ãƒªã‚»ãƒƒãƒˆ
    const vowels = ['a', 'i', 'u', 'e', 'o']
    vowels.forEach(vowel => {
      if (available.includes(vowel)) {
        try {
          expressionManager.setValue(vowel, 0)
        } catch (error) {
          // Skip silently
        }
      }
    })

    // æŒ‡å®šã•ã‚ŒãŸéŸ³éŸ»ã‚’è¨­å®š
    if (phoneme !== 'silence' && vowels.includes(phoneme)) {
      try {
        if (available.includes(phoneme)) {
          expressionManager.setValue(phoneme, intensity)
          if (!this.alternativeLogShown) {
            console.log(`[VRMAnimationController] Using legacy vowel '${phoneme}' expression for lip sync`)
            this.alternativeLogShown = true
          }
        }
      } catch (error) {
        console.error(`[VRMAnimationController] Error setting expression '${phoneme}':`, error)
      }
    }
  }
  
  /**
   * ä»£æ›¿ã®å£ã®å‹•ãã‚’è¨­å®šï¼ˆæ¯éŸ³è¡¨æƒ…ãŒãªã„å ´åˆï¼‰
   */
  private setAlternativeMouthShape(phoneme: string, intensity: number): void {
    const expressionManager = this.vrm.expressionManager!
    
    // neutralã¾ãŸã¯ä»–ã®è¡¨æƒ…ã‚’ä½¿ã£ã¦å£ã®å‹•ãã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    try {
      // è©±ã—ã¦ã„ã‚‹æ™‚ã¯è»½ãå£ã‚’é–‹ã‘ã‚‹å‹•ä½œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
      if (phoneme !== 'silence') {
        // happyã‚„surprisedãªã©ã€å£ãŒé–‹ãè¡¨æƒ…ãŒã‚ã‚Œã°ä½¿ç”¨
        if (expressionManager.getExpressionTrackName('happy')) {
          expressionManager.setValue('happy', intensity * 0.3) // è»½ãç¬‘é¡”ã§å£ã‚’é–‹ã‘ã‚‹
          if (!this.alternativeLogShown) {
            console.log('[VRMAnimationController] Using happy expression for lip sync')
            this.alternativeLogShown = true
          }
        } else if (expressionManager.getExpressionTrackName('surprised')) {
          expressionManager.setValue('surprised', intensity * 0.2) // è»½ãé©šã„ãŸè¡¨æƒ…
          if (!this.alternativeLogShown) {
            console.log('[VRMAnimationController] Using surprised expression for lip sync')
            this.alternativeLogShown = true
          }
        }
      } else {
        // å£ã‚’é–‰ã˜ã‚‹
        if (expressionManager.getExpressionTrackName('happy')) {
          expressionManager.setValue('happy', 0)
        }
        if (expressionManager.getExpressionTrackName('surprised')) {
          expressionManager.setValue('surprised', 0)
        }
      }
    } catch (error) {
      console.log('[VRMAnimationController] Alternative mouth shape failed:', error)
    }
  }

  /**
   * ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚’åœæ­¢
   */
  stopSpeaking(): void {
    this.lipSync.stop()
    this.isSpeaking = false
    this.setMouthShape('silence', 0)
  }

  /**
   * è©±ã—ã¦ã„ã‚‹æ™‚ã®ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼
   * @param intensity - è©±ã™å¼·åº¦ï¼ˆ0-1ï¼‰
   */
  private updateSpeakingGestures(intensity: number): void {
    if (!this.vrm.humanoid) return

    try {
      // é ­ã®å‹•ãï¼ˆã‚ˆã‚Šè‡ªç„¶ã«ï¼‰
      const head = this.vrm.humanoid.getNormalizedBoneNode('head')
      if (head) {
        const headMovementY = Math.sin(this.clock * 0.008) * intensity * 0.08
        const headMovementX = Math.cos(this.clock * 0.012) * intensity * 0.04
        const headMovementZ = Math.sin(this.clock * 0.015) * intensity * 0.03
        
        head.rotation.y += headMovementY
        head.rotation.x += headMovementX
        head.rotation.z += headMovementZ
      }

      // è‚©ã®å‹•ã
      const leftShoulder = this.vrm.humanoid.getNormalizedBoneNode('leftShoulder')
      const rightShoulder = this.vrm.humanoid.getNormalizedBoneNode('rightShoulder')
      
      if (leftShoulder) {
        const shoulderMovement = Math.sin(this.clock * 0.006) * intensity * 0.05
        leftShoulder.rotation.z = shoulderMovement
      }
      
      if (rightShoulder) {
        const shoulderMovement = Math.cos(this.clock * 0.006) * intensity * 0.05
        rightShoulder.rotation.z = -shoulderMovement
      }

      // è…•ã®ã‚ˆã‚Šæ´»ç™ºãªå‹•ã
      const leftUpperArm = this.vrm.humanoid.getNormalizedBoneNode('leftUpperArm')
      const rightUpperArm = this.vrm.humanoid.getNormalizedBoneNode('rightUpperArm')
      
      if (leftUpperArm) {
        const armGesture = Math.sin(this.clock * 0.005) * intensity * 0.15
        leftUpperArm.rotation.x += armGesture
        leftUpperArm.rotation.z += armGesture * 0.5
      }
      
      if (rightUpperArm) {
        const armGesture = Math.cos(this.clock * 0.005) * intensity * 0.15
        rightUpperArm.rotation.x += armGesture
        rightUpperArm.rotation.z -= armGesture * 0.5
      }

    } catch (error) {
      // Speaking gestures failed, skip silently
    }
  }

  /**
   * è¶³ã®å¾®ç´°ãªé‡å¿ƒç§»å‹•
   */
  private updateLegShift(): void {
    if (!this.vrm.humanoid) return

    try {
      const leftUpperLeg = this.vrm.humanoid.getNormalizedBoneNode('leftUpperLeg')
      const rightUpperLeg = this.vrm.humanoid.getNormalizedBoneNode('rightUpperLeg')

      // é‡å¿ƒç§»å‹•ï¼ˆå·¦å³ã®è¶³ã«äº¤äº’ã«ä½“é‡ã‚’ã‹ã‘ã‚‹ï¼‰
      const weightShift = Math.sin(this.clock * this.legShiftSpeed) * this.legShiftIntensity

      if (leftUpperLeg) {
        leftUpperLeg.rotation.z = weightShift * 0.5
      }

      if (rightUpperLeg) {
        rightUpperLeg.rotation.z = -weightShift * 0.5
      }
    } catch (error) {
      // Leg shift animation failed, skip silently
    }
  }

  /**
   * åˆ©ç”¨å¯èƒ½ãªè¡¨æƒ…ãƒªã‚¹ãƒˆã‚’å–å¾—
   */
  getAvailableExpressions(): string[] {
    const expressionManager = this.vrm.expressionManager
    if (!expressionManager) return []

    const expressions: string[] = []
    const commonExpressions = [
      'neutral', 'happy', 'sad', 'angry', 'surprised', 'relaxed',
      'blink', 'a', 'i', 'u', 'e', 'o'
    ]

    commonExpressions.forEach(expr => {
      try {
        if (expressionManager.getExpressionTrackName && expressionManager.getExpressionTrackName(expr)) {
          expressions.push(expr)
        }
      } catch (error) {
        // Expression not available, skip silently
      }
    })

    // æ¯éŸ³ã®è¡¨æƒ…ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
    const vowels = ['a', 'i', 'u', 'e', 'o']
    const availableVowels = vowels.filter(vowel => expressions.includes(vowel))
    console.log('[VRMAnimationController] Available vowel expressions:', availableVowels)
    console.log('[VRMAnimationController] All available expressions:', expressions)
    
    // VRMæ¨™æº–çš„ãªå£ã®è¡¨æƒ…ã‚‚ç¢ºèª
    const standardMouthExpressions = ['aa', 'ih', 'ou', 'ee', 'oh', 'neutral']
    const availableStandardMouthExpressions = standardMouthExpressions.filter(expr => expressions.includes(expr))
    console.log('[VRMAnimationController] Available standard mouth expressions:', availableStandardMouthExpressions)
    
    // æ—§å½¢å¼ã®æ¯éŸ³è¡¨æƒ…ã‚‚ç¢ºèª
    const legacyVowels = ['a', 'i', 'u', 'e', 'o']
    const availableLegacyVowels = legacyVowels.filter(vowel => expressions.includes(vowel))
    console.log('[VRMAnimationController] Available legacy vowel expressions:', availableLegacyVowels)

    return expressions
  }
}
```

## 4. LipSync (éŸ³éŸ»è§£æã¨ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³)

**ãƒ•ã‚¡ã‚¤ãƒ«**: `lib/lip-sync.ts`

```typescript
/**
 * VRMç”¨ã®ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚·ã‚¹ãƒ†ãƒ 
 * æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³éŸ»ã«å¤‰æ›ã—ã¦VRMã®è¡¨æƒ…ã‚’åˆ¶å¾¡ã™ã‚‹
 */

export interface PhonemeData {
  phoneme: string    // éŸ³éŸ» (a, i, u, e, o, silence)
  duration: number   // ç¶™ç¶šæ™‚é–“ (ms)
  intensity: number  // å¼·åº¦ (0-1)
}

export interface LipSyncOptions {
  speed: number        // è©±ã™é€Ÿåº¦ (æ–‡å­—/ç§’)
  intensity: number    // å£ã®å‹•ãã®å¼·åº¦
  pauseDuration: number // å¥èª­ç‚¹ã§ã®ä¼‘æ­¢æ™‚é–“ (ms)
}

export class LipSync {
  private isPlaying: boolean = false
  private animationId: number | null = null
  private onPhonemeChange: ((phoneme: string, intensity: number) => void) | null = null
  private onComplete: (() => void) | null = null

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³éŸ»ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
   */
  static textToPhonemes(text: string, options: LipSyncOptions = {
    speed: 8,           // 8æ–‡å­—/ç§’
    intensity: 0.7,     // 70%ã®å¼·åº¦
    pauseDuration: 300  // 300msä¼‘æ­¢
  }): PhonemeData[] {
    const phonemes: PhonemeData[] = []
    const charDuration = 1000 / options.speed // 1æ–‡å­—ã‚ãŸã‚Šã®æ™‚é–“

    // æ—¥æœ¬èªã®éŸ³éŸ»ãƒãƒƒãƒ”ãƒ³ã‚°
    const phonemeMap: { [key: string]: string } = {
      'ã‚': 'a', 'ã‹': 'a', 'ã•': 'a', 'ãŸ': 'a', 'ãª': 'a', 'ã¯': 'a', 'ã¾': 'a', 'ã‚„': 'a', 'ã‚‰': 'a', 'ã‚': 'a',
      'ã„': 'i', 'ã': 'i', 'ã—': 'i', 'ã¡': 'i', 'ã«': 'i', 'ã²': 'i', 'ã¿': 'i', 'ã‚Š': 'i',
      'ã†': 'u', 'ã': 'u', 'ã™': 'u', 'ã¤': 'u', 'ã¬': 'u', 'ãµ': 'u', 'ã‚€': 'u', 'ã‚†': 'u', 'ã‚‹': 'u',
      'ãˆ': 'e', 'ã‘': 'e', 'ã›': 'e', 'ã¦': 'e', 'ã­': 'e', 'ã¸': 'e', 'ã‚': 'e', 'ã‚Œ': 'e',
      'ãŠ': 'o', 'ã“': 'o', 'ã': 'o', 'ã¨': 'o', 'ã®': 'o', 'ã»': 'o', 'ã‚‚': 'o', 'ã‚ˆ': 'o', 'ã‚': 'o', 'ã‚’': 'o'
    }

    for (let i = 0; i < text.length; i++) {
      const char = text[i]
      
      // å¥èª­ç‚¹ã§ã®ä¼‘æ­¢
      if ('ã€‚ï¼ï¼Ÿã€ï¼ï¼Œ'.includes(char)) {
        phonemes.push({
          phoneme: 'silence',
          duration: options.pauseDuration,
          intensity: 0
        })
        continue
      }
      
      // ã‚¹ãƒšãƒ¼ã‚¹ã‚„æ”¹è¡Œã§ã®çŸ­ã„ä¼‘æ­¢
      if (/\s/.test(char)) {
        phonemes.push({
          phoneme: 'silence',
          duration: charDuration * 0.5,
          intensity: 0
        })
        continue
      }

      // éŸ³éŸ»ãƒãƒƒãƒ”ãƒ³ã‚°
      let phoneme = phonemeMap[char] || 'a' // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯'a'
      
      // é€£ç¶šã™ã‚‹åŒã˜éŸ³éŸ»ã®å ´åˆã€å¼·åº¦ã‚’å¤‰ãˆã‚‹
      const prevPhoneme = phonemes[phonemes.length - 1]
      let intensity = options.intensity
      if (prevPhoneme && prevPhoneme.phoneme === phoneme) {
        intensity *= 0.8 // å°‘ã—å¼±ãã™ã‚‹
      }

      phonemes.push({
        phoneme,
        duration: charDuration,
        intensity
      })
    }

    return phonemes
  }

  /**
   * ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹
   */
  play(
    phonemes: PhonemeData[],
    onPhonemeChange: (phoneme: string, intensity: number) => void,
    onComplete?: () => void
  ): void {
    if (this.isPlaying) {
      this.stop()
    }

    this.isPlaying = true
    this.onPhonemeChange = onPhonemeChange
    this.onComplete = onComplete || null

    let currentIndex = 0
    let startTime = performance.now()

    const animate = (currentTime: number) => {
      if (!this.isPlaying) return

      const elapsedTime = currentTime - startTime

      // ç¾åœ¨ã®éŸ³éŸ»ã‚’è¦‹ã¤ã‘ã‚‹
      let accumulatedTime = 0
      let currentPhoneme: PhonemeData | null = null
      let phonemeIndex = -1

      for (let i = 0; i < phonemes.length; i++) {
        if (elapsedTime >= accumulatedTime && elapsedTime < accumulatedTime + phonemes[i].duration) {
          currentPhoneme = phonemes[i]
          phonemeIndex = i
          break
        }
        accumulatedTime += phonemes[i].duration
      }

      if (currentPhoneme) {
        // å‰å›ã¨ç•°ãªã‚‹éŸ³éŸ»ã®å ´åˆã®ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°ï¼ˆãƒ­ã‚°ã¯å‰Šé™¤ï¼‰
        if (phonemeIndex !== currentIndex) {
          currentIndex = phonemeIndex
        }
        
        // éŸ³éŸ»ã®å¤‰åŒ–ã‚’é€šçŸ¥
        if (this.onPhonemeChange) {
          const intensity = currentPhoneme.phoneme === 'silence' ? 0 : currentPhoneme.intensity
          this.onPhonemeChange(currentPhoneme.phoneme, intensity)
        }
      } else {
        // ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†
        this.stop()
        if (this.onComplete) {
          this.onComplete()
        }
        return
      }

      this.animationId = requestAnimationFrame(animate)
    }

    this.animationId = requestAnimationFrame(animate)
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚’é–‹å§‹
   */
  playText(
    text: string,
    onPhonemeChange: (phoneme: string, intensity: number) => void,
    onComplete?: () => void,
    options?: Partial<LipSyncOptions>
  ): void {
    const defaultOptions: LipSyncOptions = {
      speed: 8,
      intensity: 0.7,
      pauseDuration: 300
    }
    
    const finalOptions = { ...defaultOptions, ...options }
    const phonemes = LipSync.textToPhonemes(text, finalOptions)
    console.log(`[LipSync] Generated ${phonemes.length} phonemes for text: "${text.substring(0, 20)}..."`)
    this.play(phonemes, onPhonemeChange, onComplete)
  }

  /**
   * ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ã‚’åœæ­¢
   */
  stop(): void {
    this.isPlaying = false
    if (this.animationId) {
      cancelAnimationFrame(this.animationId)
      this.animationId = null
    }
    
    // å£ã‚’é–‰ã˜ã‚‹
    if (this.onPhonemeChange) {
      this.onPhonemeChange('silence', 0)
    }
  }
}
```

## 5. éŸ³å£°åˆæˆçµ±åˆ (VOICEVOX + Web Speech API)

**ãƒ•ã‚¡ã‚¤ãƒ«**: `hooks/useUnifiedVoiceSynthesis.ts`

```typescript
/**
 * Unified Voice Synthesis React Hook
 * VOICEVOX ã¨ Web Speech API ã‚’çµ±åˆã—ãŸéŸ³å£°åˆæˆãƒ•ãƒƒã‚¯
 */

import { useState, useEffect, useRef, useCallback } from 'react'
import type { Emotion } from '@asd-aituber/types'
import { 
  UnifiedVoiceSynthesis,
  unifiedVoiceSynthesis,
  type VoiceEngine,
  type UnifiedVoiceOptions,
  type VoiceEngineStatus
} from '@/lib/unified-voice-synthesis'
import { type VoicevoxSpeaker } from '@/lib/voicevox-client'
import { type SpeechSynthesisCallbacks } from '@/lib/speech-synthesis'

export interface UseUnifiedVoiceSynthesisOptions {
  preferredEngine?: VoiceEngine
  defaultSpeaker?: string | number
  defaultEmotion?: Emotion
  defaultMode?: 'asd' | 'nt'
  volume?: number
  callbacks?: SpeechSynthesisCallbacks
}

export interface UseUnifiedVoiceSynthesisReturn {
  // éŸ³å£°åˆæˆåˆ¶å¾¡
  speak: (text: string, options?: Partial<UnifiedVoiceOptions>) => Promise<boolean>
  stop: () => void
  pause: () => void
  resume: () => void
  
  // çŠ¶æ…‹
  isSpeaking: boolean
  isLoading: boolean
  error: string | null
  
  // ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
  currentEngine: VoiceEngine
  setPreferredEngine: (engine: VoiceEngine) => void
  engineStatus: VoiceEngineStatus | null
  
  // VOICEVOXè¨­å®š
  voicevoxSpeakers: VoicevoxSpeaker[]
  selectedSpeaker: string | number
  setSelectedSpeaker: (speaker: string | number) => void
  
  // Web Speech APIè¨­å®š
  webSpeechVoices: SpeechSynthesisVoice[]
  selectedWebVoice: SpeechSynthesisVoice | null
  setSelectedWebVoice: (voice: SpeechSynthesisVoice | null) => void
  
  // å…±é€šè¨­å®š
  volume: number
  setVolume: (volume: number) => void
  emotion: Emotion
  setEmotion: (emotion: Emotion) => void
  mode: 'asd' | 'nt'
  setMode: (mode: 'asd' | 'nt') => void
  
  // ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
  refreshEngines: () => Promise<void>
  testVoice: (text?: string) => Promise<boolean>
}

export function useUnifiedVoiceSynthesis(
  options: UseUnifiedVoiceSynthesisOptions = {}
): UseUnifiedVoiceSynthesisReturn {
  const {
    preferredEngine = 'auto',
    defaultSpeaker = '46',
    defaultEmotion = 'neutral',
    defaultMode = 'nt',
    volume: initialVolume = 1.0,
    callbacks: externalCallbacks = {}
  } = options

  // çŠ¶æ…‹ç®¡ç†
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [isLoading, setIsLoading] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [currentEngine, setCurrentEngine] = useState<VoiceEngine>(preferredEngine)
  const [engineStatus, setEngineStatus] = useState<VoiceEngineStatus | null>(null)
  
  // VOICEVOXè¨­å®š
  const [voicevoxSpeakers, setVoicevoxSpeakers] = useState<VoicevoxSpeaker[]>([])
  const [selectedSpeaker, setSelectedSpeaker] = useState<string | number>(defaultSpeaker)
  
  // Web Speech APIè¨­å®š
  const [webSpeechVoices, setWebSpeechVoices] = useState<SpeechSynthesisVoice[]>([])
  const [selectedWebVoice, setSelectedWebVoice] = useState<SpeechSynthesisVoice | null>(null)
  
  // å…±é€šè¨­å®š
  const [volume, setVolume] = useState(initialVolume)
  const [emotion, setEmotion] = useState<Emotion>(defaultEmotion)
  const [mode, setMode] = useState<'asd' | 'nt'>(defaultMode)

  const isMountedRef = useRef(true)
  const synthesizerRef = useRef<UnifiedVoiceSynthesis>(unifiedVoiceSynthesis)

  // åˆæœŸåŒ–
  useEffect(() => {
    const initializeEngines = async () => {
      setIsLoading(true)
      try {
        await refreshEngines()
      } catch (error) {
        console.error('Failed to initialize voice engines:', error)
        setError('Failed to initialize voice engines')
      } finally {
        setIsLoading(false)
      }
    }

    initializeEngines()

    return () => {
      isMountedRef.current = false
      synthesizerRef.current.destroy()
    }
  }, [])

  // ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¨­å®š
  useEffect(() => {
    const callbacks: SpeechSynthesisCallbacks = {
      onStart: () => {
        if (isMountedRef.current) {
          setIsSpeaking(true)
          setError(null)
        }
        externalCallbacks.onStart?.()
      },
      onEnd: () => {
        if (isMountedRef.current) {
          setIsSpeaking(false)
        }
        externalCallbacks.onEnd?.()
      },
      onPause: () => {
        externalCallbacks.onPause?.()
      },
      onResume: () => {
        externalCallbacks.onResume?.()
      },
      onError: (errorMessage) => {
        if (isMountedRef.current) {
          setError(errorMessage)
          setIsSpeaking(false)
        }
        externalCallbacks.onError?.(errorMessage)
      },
      onBoundary: externalCallbacks.onBoundary,
      onWord: externalCallbacks.onWord
    }

    // ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ speak é–¢æ•°å†…ã§è¨­å®šã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯ä¿å­˜ã®ã¿
  }, [externalCallbacks])

  // ã‚¨ãƒ³ã‚¸ãƒ³çŠ¶æ…‹ã®æ›´æ–°
  const refreshEngines = useCallback(async () => {
    try {
      const status = await synthesizerRef.current.getEngineStatus()
      
      if (isMountedRef.current) {
        setEngineStatus(status)
        setVoicevoxSpeakers(status.voicevox.speakers)
        setWebSpeechVoices(status.webspeech.voices)
        
        // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã®è¨­å®š
        if (status.webspeech.voices.length > 0 && !selectedWebVoice) {
          const japaneseVoice = status.webspeech.voices.find(v => 
            v.lang.includes('ja') || v.lang.includes('JP')
          )
          setSelectedWebVoice(japaneseVoice || status.webspeech.voices[0])
        }
      }
    } catch (error) {
      console.error('Failed to refresh voice engines:', error)
      if (isMountedRef.current) {
        setError('Failed to refresh voice engines')
      }
    }
  }, [selectedWebVoice])

  // éŸ³å£°åˆæˆå®Ÿè¡Œ
  const speak = useCallback(async (
    text: string, 
    overrideOptions: Partial<UnifiedVoiceOptions> = {}
  ): Promise<boolean> => {
    if (!text.trim()) {
      setError('Text cannot be empty')
      return false
    }

    setError(null)

    const options: UnifiedVoiceOptions = {
      text,
      emotion,
      mode,
      engine: currentEngine,
      speaker: selectedSpeaker,
      voice: selectedWebVoice,
      lang: 'ja-JP',
      volume,
      callbacks: {
        onStart: () => {
          if (isMountedRef.current) {
            setIsSpeaking(true)
            setError(null)
          }
          externalCallbacks.onStart?.()
        },
        onEnd: () => {
          if (isMountedRef.current) {
            setIsSpeaking(false)
          }
          externalCallbacks.onEnd?.()
        },
        onError: (errorMessage) => {
          if (isMountedRef.current) {
            setError(errorMessage)
            setIsSpeaking(false)
          }
          externalCallbacks.onError?.(errorMessage)
        },
        onPause: externalCallbacks.onPause,
        onResume: externalCallbacks.onResume,
        onBoundary: externalCallbacks.onBoundary,
        onWord: externalCallbacks.onWord
      },
      ...overrideOptions
    }

    try {
      return await synthesizerRef.current.speak(options)
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Voice synthesis failed'
      setError(errorMessage)
      return false
    }
  }, [
    emotion, 
    mode, 
    currentEngine, 
    selectedSpeaker, 
    selectedWebVoice, 
    volume, 
    externalCallbacks
  ])

  // åœæ­¢
  const stop = useCallback(() => {
    synthesizerRef.current.stop()
    setIsSpeaking(false)
  }, [])

  // ä¸€æ™‚åœæ­¢
  const pause = useCallback(() => {
    synthesizerRef.current.pause()
  }, [])

  // å†é–‹
  const resume = useCallback(() => {
    synthesizerRef.current.resume()
  }, [])

  // å„ªå…ˆã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®š
  const setPreferredEngine = useCallback((engine: VoiceEngine) => {
    setCurrentEngine(engine)
    synthesizerRef.current.setPreferredEngine(engine)
  }, [])

  // éŸ³å£°ãƒ†ã‚¹ãƒˆ
  const testVoice = useCallback(async (text: string = 'ã“ã‚“ã«ã¡ã¯ã€‚éŸ³å£°ãƒ†ã‚¹ãƒˆã§ã™ã€‚') => {
    return await speak(text)
  }, [speak])

  return {
    // éŸ³å£°åˆæˆåˆ¶å¾¡
    speak,
    stop,
    pause,
    resume,
    
    // çŠ¶æ…‹
    isSpeaking,
    isLoading,
    error,
    
    // ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
    currentEngine,
    setPreferredEngine,
    engineStatus,
    
    // VOICEVOXè¨­å®š
    voicevoxSpeakers,
    selectedSpeaker,
    setSelectedSpeaker,
    
    // Web Speech APIè¨­å®š
    webSpeechVoices,
    selectedWebVoice,
    setSelectedWebVoice,
    
    // å…±é€šè¨­å®š
    volume,
    setVolume,
    emotion,
    setEmotion,
    mode,
    setMode,
    
    // ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    refreshEngines,
    testVoice
  }
}

/**
 * ç°¡æ˜“çµ±åˆéŸ³å£°åˆæˆãƒ•ãƒƒã‚¯
 */
export function useSimpleUnifiedVoice(defaultOptions?: UseUnifiedVoiceSynthesisOptions) {
  const { speak, stop, isSpeaking, error, currentEngine } = useUnifiedVoiceSynthesis({
    preferredEngine: 'auto',
    ...defaultOptions
  })

  return { speak, stop, isSpeaking, error, currentEngine }
}
```

## 6. çµ±åˆéŸ³å£°åˆæˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª

**ãƒ•ã‚¡ã‚¤ãƒ«**: `lib/unified-voice-synthesis.ts`

```typescript
/**
 * çµ±åˆéŸ³å£°åˆæˆã‚·ã‚¹ãƒ†ãƒ 
 * VOICEVOX ã¨ Web Speech API ã‚’çµ±åˆã—ã€è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã‚’æä¾›
 */

import type { Emotion } from '@asd-aituber/types'
import { VoicevoxClient, type VoicevoxSpeaker } from './voicevox-client'
import { speechSynthesis, type SpeechSynthesisCallbacks } from './speech-synthesis'

export type VoiceEngine = 'voicevox' | 'webspeech' | 'auto'

export interface UnifiedVoiceOptions {
  text: string
  emotion?: Emotion
  mode?: 'asd' | 'nt'
  engine?: VoiceEngine
  speaker?: string | number
  voice?: SpeechSynthesisVoice | null
  lang?: string
  volume?: number
  callbacks?: SpeechSynthesisCallbacks
}

export interface VoiceEngineStatus {
  voicevox: {
    available: boolean
    speakers: VoicevoxSpeaker[]
    error?: string
  }
  webspeech: {
    available: boolean
    voices: SpeechSynthesisVoice[]
  }
}

/**
 * çµ±åˆéŸ³å£°åˆæˆã‚¯ãƒ©ã‚¹
 */
export class UnifiedVoiceSynthesis {
  private voicevoxClient: VoicevoxClient
  private preferredEngine: VoiceEngine = 'auto'
  
  constructor() {
    this.voicevoxClient = new VoicevoxClient()
  }

  /**
   * éŸ³å£°åˆæˆã‚’å®Ÿè¡Œ
   */
  async speak(options: UnifiedVoiceOptions): Promise<boolean> {
    const {
      text,
      emotion = 'neutral',
      mode = 'nt',
      engine = this.preferredEngine,
      speaker = '46',
      voice = null,
      lang = 'ja-JP',
      volume = 1.0,
      callbacks = {}
    } = options

    // ã‚¨ãƒ³ã‚¸ãƒ³ã®æ±ºå®š
    const selectedEngine = await this.selectEngine(engine)
    
    try {
      switch (selectedEngine) {
        case 'voicevox':
          return await this.speakWithVoicevox(text, {
            speaker,
            emotion,
            mode,
            volume,
            callbacks
          })
          
        case 'webspeech':
          return await this.speakWithWebSpeech(text, {
            voice,
            lang,
            volume,
            callbacks
          })
          
        default:
          throw new Error(`Unsupported engine: ${selectedEngine}`)
      }
    } catch (error) {
      console.error(`Voice synthesis failed with ${selectedEngine}:`, error)
      
      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ¥ã®ã‚¨ãƒ³ã‚¸ãƒ³ã‚’è©¦è¡Œ
      if (engine === 'auto' && selectedEngine !== 'webspeech') {
        console.log('Falling back to Web Speech API')
        return await this.speakWithWebSpeech(text, {
          voice,
          lang,
          volume,
          callbacks
        })
      }
      
      callbacks.onError?.(`Voice synthesis failed: ${error instanceof Error ? error.message : 'Unknown error'}`)
      return false
    }
  }

  /**
   * VOICEVOXã§éŸ³å£°åˆæˆ
   */
  private async speakWithVoicevox(
    text: string,
    options: {
      speaker: string | number
      emotion: Emotion
      mode: 'asd' | 'nt'
      volume: number
      callbacks: SpeechSynthesisCallbacks
    }
  ): Promise<boolean> {
    const { speaker, emotion, mode, volume, callbacks } = options
    
    try {
      console.log('ğŸ¤ Using VOICEVOX engine for synthesis')
      
      callbacks.onStart?.()
      
      const audioUrl = await this.voicevoxClient.generateSpeech(text, {
        speaker: typeof speaker === 'string' ? parseInt(speaker) : speaker,
        emotion,
        mode,
        volume
      })
      
      // éŸ³å£°ã‚’å†ç”Ÿ
      return new Promise((resolve) => {
        const audio = new Audio(audioUrl)
        audio.volume = volume
        
        audio.onended = () => {
          callbacks.onEnd?.()
          resolve(true)
        }
        
        audio.onerror = (error) => {
          console.error('Audio playback error:', error)
          callbacks.onError?.('Audio playback failed')
          resolve(false)
        }
        
        audio.play().catch(error => {
          console.error('Failed to play audio:', error)
          callbacks.onError?.('Failed to play audio')
          resolve(false)
        })
      })
      
    } catch (error) {
      console.error('VOICEVOX synthesis failed:', error)
      throw error
    }
  }

  /**
   * Web Speech APIã§éŸ³å£°åˆæˆ
   */
  private async speakWithWebSpeech(
    text: string,
    options: {
      voice: SpeechSynthesisVoice | null
      lang: string
      volume: number
      callbacks: SpeechSynthesisCallbacks
    }
  ): Promise<boolean> {
    const { voice, lang, volume, callbacks } = options
    
    console.log('ğŸ—£ï¸ Using webspeech engine for synthesis')
    
    return speechSynthesis.speak(text, {
      voice,
      lang,
      volume,
      rate: 1.0,
      pitch: 1.0,
      callbacks
    })
  }

  /**
   * ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ³ã‚¸ãƒ³ã‚’é¸æŠ
   */
  private async selectEngine(preferredEngine: VoiceEngine): Promise<VoiceEngine> {
    if (preferredEngine === 'voicevox') {
      const isVoicevoxAvailable = await this.voicevoxClient.isAvailable()
      if (isVoicevoxAvailable) {
        return 'voicevox'
      } else {
        throw new Error('VOICEVOX is not available')
      }
    }
    
    if (preferredEngine === 'webspeech') {
      return 'webspeech'
    }
    
    // auto: VOICEVOXã‚’å„ªå…ˆã—ã€åˆ©ç”¨ã§ããªã„å ´åˆã¯Web Speech APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    const isVoicevoxAvailable = await this.voicevoxClient.isAvailable()
    return isVoicevoxAvailable ? 'voicevox' : 'webspeech'
  }

  /**
   * ã‚¨ãƒ³ã‚¸ãƒ³ã®çŠ¶æ…‹ã‚’å–å¾—
   */
  async getEngineStatus(): Promise<VoiceEngineStatus> {
    const [voicevoxStatus, webSpeechVoices] = await Promise.all([
      this.getVoicevoxStatus(),
      this.getWebSpeechVoices()
    ])

    return {
      voicevox: voicevoxStatus,
      webspeech: {
        available: webSpeechVoices.length > 0,
        voices: webSpeechVoices
      }
    }
  }

  /**
   * VOICEVOXã®çŠ¶æ…‹ã‚’å–å¾—
   */
  private async getVoicevoxStatus() {
    try {
      const isAvailable = await this.voicevoxClient.isAvailable()
      if (isAvailable) {
        const speakers = await this.voicevoxClient.getSpeakers()
        return {
          available: true,
          speakers
        }
      } else {
        return {
          available: false,
          speakers: [],
          error: 'VOICEVOX server not accessible'
        }
      }
    } catch (error) {
      return {
        available: false,
        speakers: [],
        error: error instanceof Error ? error.message : 'Unknown error'
      }
    }
  }

  /**
   * Web Speech APIã®éŸ³å£°ãƒªã‚¹ãƒˆã‚’å–å¾—
   */
  private async getWebSpeechVoices(): Promise<SpeechSynthesisVoice[]> {
    return new Promise((resolve) => {
      if (typeof window === 'undefined' || !window.speechSynthesis) {
        resolve([])
        return
      }

      const voices = window.speechSynthesis.getVoices()
      if (voices.length > 0) {
        resolve(voices)
      } else {
        // åˆå›èª­ã¿è¾¼ã¿æ™‚ã¯éŸ³å£°ãƒªã‚¹ãƒˆãŒç©ºã®å ´åˆãŒã‚ã‚‹ãŸã‚ã€ã‚¤ãƒ™ãƒ³ãƒˆã‚’å¾…ã¤
        const onVoicesChanged = () => {
          const voices = window.speechSynthesis.getVoices()
          resolve(voices)
          window.speechSynthesis.removeEventListener('voiceschanged', onVoicesChanged)
        }
        
        window.speechSynthesis.addEventListener('voiceschanged', onVoicesChanged)
        
        // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        setTimeout(() => {
          window.speechSynthesis.removeEventListener('voiceschanged', onVoicesChanged)
          resolve([])
        }, 1000)
      }
    })
  }

  /**
   * å„ªå…ˆã‚¨ãƒ³ã‚¸ãƒ³ã‚’è¨­å®š
   */
  setPreferredEngine(engine: VoiceEngine): void {
    this.preferredEngine = engine
  }

  /**
   * éŸ³å£°åˆæˆã‚’åœæ­¢
   */
  stop(): void {
    // Web Speech API ã®åœæ­¢
    if (typeof window !== 'undefined' && window.speechSynthesis) {
      window.speechSynthesis.cancel()
    }
    
    // VOICEVOX ã®åœæ­¢ã¯ç¾åœ¨ã®ã¨ã“ã‚ã€audioè¦ç´ ã®stop()ãŒå¿…è¦
    // å®Ÿè£…ã¯å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã§ç®¡ç†
  }

  /**
   * éŸ³å£°åˆæˆã‚’ä¸€æ™‚åœæ­¢
   */
  pause(): void {
    if (typeof window !== 'undefined' && window.speechSynthesis) {
      window.speechSynthesis.pause()
    }
  }

  /**
   * éŸ³å£°åˆæˆã‚’å†é–‹
   */
  resume(): void {
    if (typeof window !== 'undefined' && window.speechSynthesis) {
      window.speechSynthesis.resume()
    }
  }

  /**
   * ãƒªã‚½ãƒ¼ã‚¹ã®è§£æ”¾
   */
  destroy(): void {
    this.stop()
  }
}

// ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
export const unifiedVoiceSynthesis = new UnifiedVoiceSynthesis()
```

---

## å•é¡Œã®è¦ç‚¹

1. **ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯å®Ÿè£…ã«åˆ‡ã‚Šæ›¿ãˆæ¸ˆã¿** - aituber-kitæ–¹å¼ã‚’æ¡ç”¨
2. **è©³ç´°ãªãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’è¿½åŠ æ¸ˆã¿** - è¡¨æƒ…ã®å­˜åœ¨ç¢ºèªã¨å€¤ã®è¨­å®šã‚’è©³ç´°ã«è¿½è·¡
3. **VRMæ›´æ–°é †åºã‚‚ç¢ºèªæ¸ˆã¿** - ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®æ›´æ–°ãŒVRMæ›´æ–°å‰ã«å®Ÿè¡Œ

æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€å®Ÿéš›ã®VRMãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã¨expressionManagerã®è©³ç´°ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚