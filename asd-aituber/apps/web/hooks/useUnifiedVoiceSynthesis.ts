/**
 * Unified Voice Synthesis React Hook
 * VOICEVOX ã¨ Web Speech API ã‚’çµ±åˆã—ãŸéŸ³å£°åˆæˆãƒ•ãƒƒã‚¯
 */

import { useState, useEffect, useRef, useCallback } from 'react'
import type { Emotion } from '@asd-aituber/types'
import { performanceMonitor } from '@/lib/performance-monitor'
import { 
  UnifiedVoiceSynthesis,
  unifiedVoiceSynthesis,
  type VoiceEngine,
  type UnifiedVoiceOptions,
  type VoiceEngineStatus
} from '@/lib/unified-voice-synthesis'
import { type VoicevoxSpeaker, type VoicevoxAudioQuery } from '@/lib/voicevox-client'
import { type SpeechSynthesisCallbacks } from '@/lib/speech-synthesis'
import { AudioContextManager } from '@/libs/audio-context-manager'

export interface UseUnifiedVoiceSynthesisOptions {
  preferredEngine?: VoiceEngine
  defaultSpeaker?: string | number
  defaultEmotion?: Emotion
  defaultMode?: 'asd' | 'nt'
  volume?: number
  callbacks?: SpeechSynthesisCallbacks
  // Phase 4: éŸ³ç´ ãƒ‡ãƒ¼ã‚¿ã¨ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªè¦ç´ ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  onLipSyncData?: (audioQuery: VoicevoxAudioQuery) => void
  onAudioReady?: (audio: HTMLAudioElement) => void
  // âœ… AudioLipSyncå¯¾å¿œã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  onAudioBufferReady?: (audioBuffer: ArrayBuffer) => void
}

export interface UseUnifiedVoiceSynthesisReturn {
  // éŸ³å£°åˆæˆåˆ¶å¾¡
  speak: (text: string, options?: Partial<UnifiedVoiceOptions>) => Promise<boolean>
  stop: () => void
  pause: () => void
  resume: () => void
  
  // çŠ¶æ…‹
  isSpeaking: boolean
  isLoading: boolean
  error: string | null
  
  // ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
  currentEngine: VoiceEngine
  setPreferredEngine: (engine: VoiceEngine) => void
  engineStatus: VoiceEngineStatus | null
  
  // VOICEVOXè¨­å®š
  voicevoxSpeakers: VoicevoxSpeaker[]
  selectedSpeaker: string | number
  setSelectedSpeaker: (speaker: string | number) => void
  
  // Web Speech APIè¨­å®š
  webSpeechVoices: SpeechSynthesisVoice[]
  selectedWebVoice: SpeechSynthesisVoice | null
  setSelectedWebVoice: (voice: SpeechSynthesisVoice | null) => void
  
  // å…±é€šè¨­å®š
  volume: number
  setVolume: (volume: number) => void
  emotion: Emotion
  setEmotion: (emotion: Emotion) => void
  mode: 'asd' | 'nt'
  setMode: (mode: 'asd' | 'nt') => void
  
  // ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
  refreshEngines: () => Promise<void>
  testVoice: (text?: string) => Promise<boolean>
}

export function useUnifiedVoiceSynthesis(
  options: UseUnifiedVoiceSynthesisOptions = {}
): UseUnifiedVoiceSynthesisReturn {
  const {
    preferredEngine = 'auto',
    defaultSpeaker = '46',
    defaultEmotion = 'neutral',
    defaultMode = 'nt',
    volume: initialVolume = 1.0,
    callbacks: externalCallbacks = {},
    onLipSyncData,
    onAudioReady,
    onAudioBufferReady
  } = options

  // çŠ¶æ…‹ç®¡ç†
  const [isSpeaking, setIsSpeaking] = useState(false)
  const [isLoading, setIsLoading] = useState(false)
  const [error, setError] = useState<string | null>(null)
  const [currentEngine, setCurrentEngine] = useState<VoiceEngine>(preferredEngine)
  const [engineStatus, setEngineStatus] = useState<VoiceEngineStatus | null>(null)
  
  // VOICEVOXè¨­å®š
  const [voicevoxSpeakers, setVoicevoxSpeakers] = useState<VoicevoxSpeaker[]>([])
  const [selectedSpeaker, setSelectedSpeaker] = useState<string | number>(defaultSpeaker)
  
  // Web Speech APIè¨­å®š
  const [webSpeechVoices, setWebSpeechVoices] = useState<SpeechSynthesisVoice[]>([])
  const [selectedWebVoice, setSelectedWebVoice] = useState<SpeechSynthesisVoice | null>(null)
  
  // å…±é€šè¨­å®š
  const [volume, setVolume] = useState(initialVolume)
  const [emotion, setEmotion] = useState<Emotion>(defaultEmotion)
  const [mode, setMode] = useState<'asd' | 'nt'>(defaultMode)

  const isMountedRef = useRef(true)
  const synthesizerRef = useRef<UnifiedVoiceSynthesis | null>(unifiedVoiceSynthesis)
  const audioManagerRef = useRef<AudioContextManager | null>(null)

  // åˆæœŸåŒ–
  useEffect(() => {
    // AudioContextManagerã®åˆæœŸåŒ–
    audioManagerRef.current = AudioContextManager.getInstance()
    
    if (!synthesizerRef.current) {
      // ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ã§åˆæœŸåŒ–
      if (typeof window !== 'undefined') {
        synthesizerRef.current = new UnifiedVoiceSynthesis()
      } else {
        return
      }
    }

    const initializeEngines = async () => {
      setIsLoading(true)
      try {
        await refreshEngines()
      } catch (error) {
        if (process.env.NODE_ENV === 'development') {
          console.error('Failed to initialize voice engines:', error)
        }
        setError('Failed to initialize voice engines')
      } finally {
        setIsLoading(false)
      }
    }

    initializeEngines()

    return () => {
      isMountedRef.current = false
      synthesizerRef.current?.destroy()
      // AudioContextManagerã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
      audioManagerRef.current?.setIsSpeaking(false)
    }
  }, [])

  // ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®è¨­å®š
  useEffect(() => {
    const callbacks: SpeechSynthesisCallbacks = {
      onStart: () => {
        if (isMountedRef.current) {
          setIsSpeaking(true)
          setError(null)
          // AudioContextManagerã«éŸ³å£°åˆæˆé–‹å§‹ã‚’é€šçŸ¥
          audioManagerRef.current?.setIsSpeaking(true)
        }
        externalCallbacks.onStart?.()
      },
      onEnd: () => {
        if (isMountedRef.current) {
          setIsSpeaking(false)
          // AudioContextManagerã«éŸ³å£°åˆæˆçµ‚äº†ã‚’é€šçŸ¥
          audioManagerRef.current?.setIsSpeaking(false)
        }
        externalCallbacks.onEnd?.()
      },
      onPause: () => {
        externalCallbacks.onPause?.()
      },
      onResume: () => {
        externalCallbacks.onResume?.()
      },
      onError: (errorMessage) => {
        if (isMountedRef.current) {
          setError(errorMessage)
          setIsSpeaking(false)
          // AudioContextManagerã«ã‚¨ãƒ©ãƒ¼æ™‚ã®çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆã‚’é€šçŸ¥
          audioManagerRef.current?.setIsSpeaking(false)
        }
        externalCallbacks.onError?.(errorMessage)
      },
      onBoundary: externalCallbacks.onBoundary,
      onWord: externalCallbacks.onWord
    }

    // ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ speak é–¢æ•°å†…ã§è¨­å®šã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯ä¿å­˜ã®ã¿
  }, [externalCallbacks])

  // ã‚¨ãƒ³ã‚¸ãƒ³çŠ¶æ…‹ã®æ›´æ–°
  const refreshEngines = useCallback(async () => {
    try {
      if (!synthesizerRef.current) return
      const status = await synthesizerRef.current.getEngineStatus()
      
      if (isMountedRef.current) {
        setEngineStatus(status)
        setVoicevoxSpeakers(status.voicevox.speakers)
        setWebSpeechVoices(status.webspeech.voices)
        
        // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéŸ³å£°ã®è¨­å®š
        if (status.webspeech.voices.length > 0 && !selectedWebVoice) {
          const japaneseVoice = status.webspeech.voices.find(v => 
            v.lang.includes('ja') || v.lang.includes('JP')
          )
          setSelectedWebVoice(japaneseVoice || status.webspeech.voices[0])
        }
      }
    } catch (error) {
      if (process.env.NODE_ENV === 'development') {
        console.error('Failed to refresh voice engines:', error)
      }
      if (isMountedRef.current) {
        setError('Failed to refresh voice engines')
      }
    }
  }, [selectedWebVoice])

  // éŸ³å£°åˆæˆå®Ÿè¡Œ
  const speak = useCallback(async (
    text: string, 
    overrideOptions: Partial<UnifiedVoiceOptions> = {}
  ): Promise<boolean> => {
    if (!text.trim()) {
      setError('Text cannot be empty')
      return false
    }

    setError(null)

    // ğŸ”§ CRITICAL: éŸ³å£°åˆæˆé–‹å§‹å‰ã«å³åº§ã«ãƒã‚¤ã‚¯ã‚’åœæ­¢ã—ã¦ã‚¨ã‚³ãƒ¼ãƒ«ãƒ¼ãƒ—ã‚’é˜²æ­¢
    setIsSpeaking(true)
    audioManagerRef.current?.setIsSpeaking(true)

    const options: UnifiedVoiceOptions = {
      text,
      emotion,
      mode,
      engine: currentEngine,
      speaker: selectedSpeaker,
      voice: selectedWebVoice,
      lang: 'ja-JP',
      volume,
      callbacks: {
        onStart: () => {
          performanceMonitor.startMeasure('voice-synthesis-processing')
          if (isMountedRef.current) {
            setError(null)
            // çŠ¶æ…‹ã¯æ—¢ã«speaké–‹å§‹æ™‚ã«è¨­å®šæ¸ˆã¿
          }
          externalCallbacks.onStart?.()
        },
        onEnd: () => {
          performanceMonitor.endMeasure('voice-synthesis-processing')
          if (isMountedRef.current) {
            setIsSpeaking(false)
            // AudioContextManagerã«éŸ³å£°åˆæˆçµ‚äº†ã‚’é€šçŸ¥
            audioManagerRef.current?.setIsSpeaking(false)
          }
          externalCallbacks.onEnd?.()
        },
        onError: (errorMessage) => {
          if (isMountedRef.current) {
            setError(errorMessage)
            setIsSpeaking(false)
            // AudioContextManagerã«ã‚¨ãƒ©ãƒ¼æ™‚ã®çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆã‚’é€šçŸ¥
            audioManagerRef.current?.setIsSpeaking(false)
          }
          externalCallbacks.onError?.(errorMessage)
        },
        onPause: externalCallbacks.onPause,
        onResume: externalCallbacks.onResume,
        onBoundary: externalCallbacks.onBoundary,
        onWord: externalCallbacks.onWord
      },
      // Phase 4: éŸ³ç´ ãƒ‡ãƒ¼ã‚¿ã¨ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªè¦ç´ ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ 
      onLipSyncData,
      onAudioReady,
      // âœ… AudioLipSyncå¯¾å¿œã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ 
      onAudioBufferReady,
      ...overrideOptions
    }

    try {
      if (!synthesizerRef.current) {
        setError('Voice synthesis not available')
        // AudioContextManagerã«çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆã‚’é€šçŸ¥
        audioManagerRef.current?.setIsSpeaking(false)
        return false
      }
      
      const result = await synthesizerRef.current.speak(options)
      
      // æˆåŠŸãƒ»å¤±æ•—ã«é–¢ã‚ã‚‰ãšã€finallyç¯€ã§çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
      if (!result) {
        audioManagerRef.current?.setIsSpeaking(false)
      }
      
      return result
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Voice synthesis failed'
      setError(errorMessage)
      setIsSpeaking(false)
      // AudioContextManagerã«ã‚¨ãƒ©ãƒ¼æ™‚ã®çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆã‚’é€šçŸ¥
      audioManagerRef.current?.setIsSpeaking(false)
      return false
    }
  }, [
    emotion, 
    mode, 
    currentEngine, 
    selectedSpeaker, 
    selectedWebVoice, 
    volume, 
    externalCallbacks,
    onLipSyncData,
    onAudioReady,
    onAudioBufferReady
  ])

  // åœæ­¢
  const stop = useCallback(() => {
    synthesizerRef.current?.stop()
    setIsSpeaking(false)
    // AudioContextManagerã«çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆã‚’é€šçŸ¥
    audioManagerRef.current?.setIsSpeaking(false)
    // Emergency stop for downstream consumers
    audioManagerRef.current?.emergencyStop()
  }, [])

  // ä¸€æ™‚åœæ­¢
  const pause = useCallback(() => {
    synthesizerRef.current?.pause()
  }, [])

  // å†é–‹
  const resume = useCallback(() => {
    synthesizerRef.current?.resume()
  }, [])

  // å„ªå…ˆã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®š
  const setPreferredEngine = useCallback((engine: VoiceEngine) => {
    setCurrentEngine(engine)
    synthesizerRef.current?.setPreferredEngine(engine)
  }, [])

  // éŸ³å£°ãƒ†ã‚¹ãƒˆ
  const testVoice = useCallback(async (text: string = 'ã“ã‚“ã«ã¡ã¯ã€‚éŸ³å£°ãƒ†ã‚¹ãƒˆã§ã™ã€‚') => {
    return await speak(text)
  }, [speak])

  return {
    // éŸ³å£°åˆæˆåˆ¶å¾¡
    speak,
    stop,
    pause,
    resume,
    
    // çŠ¶æ…‹
    isSpeaking,
    isLoading,
    error,
    
    // ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
    currentEngine,
    setPreferredEngine,
    engineStatus,
    
    // VOICEVOXè¨­å®š
    voicevoxSpeakers,
    selectedSpeaker,
    setSelectedSpeaker,
    
    // Web Speech APIè¨­å®š
    webSpeechVoices,
    selectedWebVoice,
    setSelectedWebVoice,
    
    // å…±é€šè¨­å®š
    volume,
    setVolume,
    emotion,
    setEmotion,
    mode,
    setMode,
    
    // ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    refreshEngines,
    testVoice
  }
}

/**
 * ç°¡æ˜“çµ±åˆéŸ³å£°åˆæˆãƒ•ãƒƒã‚¯
 */
export function useSimpleUnifiedVoice(defaultOptions?: UseUnifiedVoiceSynthesisOptions) {
  const { speak, stop, isSpeaking, error, currentEngine } = useUnifiedVoiceSynthesis({
    preferredEngine: 'auto',
    ...defaultOptions
  })

  return { speak, stop, isSpeaking, error, currentEngine }
}