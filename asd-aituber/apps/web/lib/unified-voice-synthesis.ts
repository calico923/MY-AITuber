/**
 * Unified Voice Synthesis System
 * VOICEVOX ã¨ Web Speech API ã®çµ±åˆéŸ³å£°åˆæˆã‚·ã‚¹ãƒ†ãƒ 
 */

import type { Emotion } from '@asd-aituber/types'
import { voicevoxClient, type VoicevoxSpeaker, type VoicevoxSynthesisOptions, type VoicevoxAudioQuery } from './voicevox-client'
import { createEmotionalVoiceOptions } from './emotion-voice-mapping'
import { SpeechSynthesisManager, type SpeechSynthesisOptions, type SpeechSynthesisCallbacks } from './speech-synthesis'

export type VoiceEngine = 'voicevox' | 'webspeech' | 'auto'

export interface UnifiedVoiceOptions {
  text: string
  emotion?: Emotion
  mode?: 'asd' | 'nt'
  engine?: VoiceEngine
  speaker?: string | number  // VOICEVOX speaker ID
  // Web Speech API options
  voice?: SpeechSynthesisVoice | null
  lang?: string
  // Common options
  volume?: number
  callbacks?: SpeechSynthesisCallbacks
  // æ–°è¦è¿½åŠ : éŸ³ç´ ãƒ‡ãƒ¼ã‚¿ã¨ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªè¦ç´ ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  onLipSyncData?: (audioQuery: VoicevoxAudioQuery) => void
  onAudioReady?: (audio: HTMLAudioElement) => void
  // âœ… AudioLipSyncå¯¾å¿œã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  onAudioBufferReady?: (audioBuffer: ArrayBuffer) => void
}

export interface VoiceEngineStatus {
  voicevox: {
    available: boolean
    speakers: VoicevoxSpeaker[]
    serverUrl: string
    error?: string
  }
  webspeech: {
    available: boolean
    voices: SpeechSynthesisVoice[]
    error?: string
  }
}

/**
 * çµ±åˆéŸ³å£°åˆæˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
 */
export class UnifiedVoiceSynthesis {
  private webSpeechManager: SpeechSynthesisManager
  private preferredEngine: VoiceEngine = 'auto'
  private currentAudio: HTMLAudioElement | null = null
  private isPlaying: boolean = false
  private callbacks: SpeechSynthesisCallbacks = {}
  // æ–°è¦è¿½åŠ : éŸ³ç´ ãƒ‡ãƒ¼ã‚¿ã¨ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªè¦ç´ ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  private onLipSyncData?: (audioQuery: VoicevoxAudioQuery) => void
  private onAudioReady?: (audio: HTMLAudioElement) => void
  // âœ… AudioLipSyncå¯¾å¿œã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
  private onAudioBufferReady?: (audioBuffer: ArrayBuffer) => void

  constructor() {
    this.webSpeechManager = new SpeechSynthesisManager()
    this.initializeVoicevox()
  }

  /**
   * VOICEVOXã®åˆæœŸåŒ–
   */
  private async initializeVoicevox(): Promise<void> {
    try {
      await voicevoxClient.checkAvailability()
      if (voicevoxClient.getIsAvailable()) {
        console.log('âœ… VOICEVOX engine initialized successfully')
      } else {
        console.warn('âš ï¸ VOICEVOX engine not available, falling back to Web Speech API')
      }
    } catch (error) {
      console.error('Failed to initialize VOICEVOX:', error)
    }
  }

  /**
   * éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã®çŠ¶æ…‹ã‚’å–å¾—
   */
  async getEngineStatus(): Promise<VoiceEngineStatus> {
    const status: VoiceEngineStatus = {
      voicevox: {
        available: false,
        speakers: [],
        serverUrl: 'http://localhost:50021'
      },
      webspeech: {
        available: false,
        voices: []
      }
    }

    // VOICEVOXçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
    try {
      status.voicevox.available = await voicevoxClient.checkAvailability()
      status.voicevox.speakers = voicevoxClient.getSpeakers()
      status.voicevox.serverUrl = 'http://localhost:50021' // TODO: è¨­å®šã‹ã‚‰å–å¾—
    } catch (error) {
      status.voicevox.error = error instanceof Error ? error.message : 'Unknown error'
    }

    // Web Speech APIçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
    try {
      status.webspeech.available = this.webSpeechManager.getIsSupported()
      status.webspeech.voices = this.webSpeechManager.getAllVoices()
    } catch (error) {
      status.webspeech.error = error instanceof Error ? error.message : 'Unknown error'
    }

    return status
  }

  /**
   * æœ€é©ãªéŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã‚’è‡ªå‹•é¸æŠ
   */
  private async selectBestEngine(requestedEngine: VoiceEngine): Promise<'voicevox' | 'webspeech'> {
    if (requestedEngine === 'voicevox') {
      return voicevoxClient.getIsAvailable() ? 'voicevox' : 'webspeech'
    }
    
    if (requestedEngine === 'webspeech') {
      return 'webspeech'
    }

    // auto ã®å ´åˆ
    const isVoicevoxAvailable = voicevoxClient.getIsAvailable()
    
    // VOICEVOXãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯å„ªå…ˆ
    if (isVoicevoxAvailable) {
      return 'voicevox'
    }
    
    return 'webspeech'
  }

  /**
   * éŸ³å£°åˆæˆã‚’å®Ÿè¡Œ
   */
  async speak(options: UnifiedVoiceOptions): Promise<boolean> {
    const {
      text,
      emotion = 'neutral',
      mode = 'nt',
      engine = 'auto',
      speaker = '46',
      voice,
      lang = 'ja-JP',
      volume = 1.0,
      callbacks = {},
      onLipSyncData,
      onAudioReady,
      onAudioBufferReady
    } = options

    // æ—¢å­˜ã®å†ç”Ÿã‚’åœæ­¢
    this.stop()

    // ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä¿å­˜
    this.callbacks = callbacks
    this.onLipSyncData = onLipSyncData
    this.onAudioReady = onAudioReady
    this.onAudioBufferReady = onAudioBufferReady

    try {
      const selectedEngine = await this.selectBestEngine(engine)
      console.log(`ğŸµ Using ${selectedEngine} engine for synthesis`)

      if (selectedEngine === 'voicevox') {
        return await this.speakWithVoicevox(text, emotion, mode, speaker, volume)
      } else {
        return await this.speakWithWebSpeech(text, emotion, mode, voice, lang, volume)
      }
    } catch (error) {
      console.error('Unified voice synthesis error:', error)
      this.callbacks.onError?.(error instanceof Error ? error.message : 'Voice synthesis failed')
      return false
    }
  }

  /**
   * VOICEVOXã§éŸ³å£°åˆæˆ
   */
  private async speakWithVoicevox(
    text: string,
    emotion: Emotion,
    mode: 'asd' | 'nt',
    speaker: string | number,
    volume: number
  ): Promise<boolean> {
    try {
      this.callbacks.onStart?.()
      this.isPlaying = true

      // æ„Ÿæƒ…ã«åŸºã¥ã„ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
      const voiceOptions = createEmotionalVoiceOptions(text, emotion, mode, speaker)
      
      console.log('ğŸµ VOICEVOX synthesis with emotion:', {
        emotion,
        mode,
        parameters: {
          speed: voiceOptions.speed,
          pitch: voiceOptions.pitch,
          intonation: voiceOptions.intonation
        }
      })

      // éŸ³å£°åˆæˆã‚’å®Ÿè¡Œï¼ˆéŸ³ç´ ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰
      const result = await voicevoxClient.synthesizeWithTiming(voiceOptions)
      
      // éŸ³ç´ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§æ¸¡ã™
      if (result.audioQuery && this.onLipSyncData) {
        console.log('ğŸ“Š Passing audioQuery data to lip sync callback')
        this.onLipSyncData(result.audioQuery)
      }
      
      // âœ… AudioBuffer ã‚’ç›´æ¥ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§æä¾›ï¼ˆaituber-kitæ–¹å¼ï¼‰
      if (this.onAudioBufferReady) {
        console.log('ğŸµ Passing ArrayBuffer directly for AudioLipSync')
        this.onAudioBufferReady(result.audioBuffer)
        // AudioLipSync ã‚’ä½¿ã†å ´åˆã¯ã€HTMLAudioElement ã¯ä½œæˆã—ãªã„
        return true
      }
      
      // å¾“æ¥æ–¹å¼ï¼ˆHTMLAudioElementï¼‰ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      const audioBlob = new Blob([result.audioBuffer], { type: 'audio/wav' })
      const audioUrl = URL.createObjectURL(audioBlob)

      // HTMLAudioElementã§å†ç”Ÿ
      this.currentAudio = new Audio(audioUrl)
      this.currentAudio.volume = volume

      // HTMLAudioElementã‚’ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§æä¾›
      if (this.onAudioReady) {
        console.log('ğŸ§ Passing HTMLAudioElement to callback')
        this.onAudioReady(this.currentAudio)
      }

      this.currentAudio.onended = () => {
        this.isPlaying = false
        this.callbacks.onEnd?.()
        URL.revokeObjectURL(audioUrl)
      }

      this.currentAudio.onerror = (error) => {
        this.isPlaying = false
        this.callbacks.onError?.('Audio playback failed')
        URL.revokeObjectURL(audioUrl)
      }

      await this.currentAudio.play()
      return true

    } catch (error) {
      this.isPlaying = false
      console.error('VOICEVOX synthesis failed:', error)
      
      // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦Web Speech APIã‚’è©¦è¡Œ
      console.log('ğŸ”„ Falling back to Web Speech API')
      return await this.speakWithWebSpeech(text, emotion, mode, undefined, 'ja-JP', volume)
    }
  }

  /**
   * Web Speech APIã§éŸ³å£°åˆæˆ
   */
  private async speakWithWebSpeech(
    text: string,
    emotion: Emotion,
    mode: 'asd' | 'nt',
    voice?: SpeechSynthesisVoice | null,
    lang: string = 'ja-JP',
    volume: number = 1.0
  ): Promise<boolean> {
    // æ„Ÿæƒ…ã«åŸºã¥ã„ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆWeb Speech APIç”¨ã«å¤‰æ›ï¼‰
    const emotionParams = this.getWebSpeechParamsForEmotion(emotion, mode)
    
    const options: SpeechSynthesisOptions = {
      voice,
      lang,
      volume,
      rate: emotionParams.rate,
      pitch: emotionParams.pitch
    }

    this.webSpeechManager.setCallbacks(this.callbacks)
    return this.webSpeechManager.speak(text, options)
  }

  /**
   * æ„Ÿæƒ…ã‚’Web Speech APIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¤‰æ›
   */
  private getWebSpeechParamsForEmotion(emotion: Emotion, mode: 'asd' | 'nt') {
    // emotion-voice-mappingã®å€¤ã‚’Web Speech APIç”¨ã«èª¿æ•´
    const baseRate = 1.0
    const basePitch = 1.0

    const adjustments = {
      asd: {
        joy: { rate: 1.05, pitch: 1.02 },
        sadness: { rate: 0.95, pitch: 0.98 },
        anger: { rate: 1.0, pitch: 1.01 },
        surprise: { rate: 1.15, pitch: 1.05 },
        fear: { rate: 0.9, pitch: 1.03 }
      },
      nt: {
        joy: { rate: 1.2, pitch: 1.08 },
        sadness: { rate: 0.8, pitch: 0.92 },
        anger: { rate: 1.1, pitch: 1.02 },
        surprise: { rate: 1.4, pitch: 1.12 },
        fear: { rate: 0.85, pitch: 1.08 }
      }
    }

    const modeAdjustments = adjustments[mode]
    const emotionAdjustment = modeAdjustments[emotion as keyof typeof modeAdjustments] || 
                              { rate: baseRate, pitch: basePitch }

    return {
      rate: emotionAdjustment.rate,
      pitch: emotionAdjustment.pitch
    }
  }

  /**
   * éŸ³å£°å†ç”Ÿã‚’åœæ­¢
   */
  stop(): void {
    if (this.currentAudio) {
      this.currentAudio.pause()
      this.currentAudio.currentTime = 0
      this.currentAudio = null
    }
    
    this.webSpeechManager.stop()
    this.isPlaying = false
  }

  /**
   * éŸ³å£°å†ç”Ÿã‚’ä¸€æ™‚åœæ­¢
   */
  pause(): void {
    if (this.currentAudio && !this.currentAudio.paused) {
      this.currentAudio.pause()
      this.callbacks.onPause?.()
    } else {
      this.webSpeechManager.pause()
    }
  }

  /**
   * éŸ³å£°å†ç”Ÿã‚’å†é–‹
   */
  resume(): void {
    if (this.currentAudio && this.currentAudio.paused) {
      this.currentAudio.play()
      this.callbacks.onResume?.()
    } else {
      this.webSpeechManager.resume()
    }
  }

  /**
   * å†ç”ŸçŠ¶æ…‹ã‚’å–å¾—
   */
  getIsSpeaking(): boolean {
    return this.isPlaying || this.webSpeechManager.getIsSpeaking()
  }

  /**
   * å„ªå…ˆã‚¨ãƒ³ã‚¸ãƒ³ã‚’è¨­å®š
   */
  setPreferredEngine(engine: VoiceEngine): void {
    this.preferredEngine = engine
  }

  /**
   * å„ªå…ˆã‚¨ãƒ³ã‚¸ãƒ³ã‚’å–å¾—
   */
  getPreferredEngine(): VoiceEngine {
    return this.preferredEngine
  }

  /**
   * åˆ©ç”¨å¯èƒ½ãªVOICEVOXè©±è€…ã‚’å–å¾—
   */
  getVoicevoxSpeakers(): VoicevoxSpeaker[] {
    return voicevoxClient.getSpeakers()
  }

  /**
   * Web Speech APIéŸ³å£°ã‚’å–å¾—
   */
  getWebSpeechVoices(): SpeechSynthesisVoice[] {
    return this.webSpeechManager.getAllVoices()
  }

  /**
   * ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
   */
  destroy(): void {
    this.stop()
    this.webSpeechManager.destroy()
    this.callbacks = {}
  }
}

/**
 * ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®çµ±åˆéŸ³å£°åˆæˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
 */
export const unifiedVoiceSynthesis = typeof window !== 'undefined' ? new UnifiedVoiceSynthesis() : null

/**
 * ä¾¿åˆ©é–¢æ•°ï¼šæ„Ÿæƒ…ã‚’è¾¼ã‚ãŸéŸ³å£°åˆæˆ
 */
export async function speakWithEmotion(
  text: string,
  emotion: Emotion = 'neutral',
  mode: 'asd' | 'nt' = 'nt',
  engine: VoiceEngine = 'auto'
): Promise<boolean> {
  if (!unifiedVoiceSynthesis) {
    console.warn('Voice synthesis not available on server side')
    return false
  }
  return await unifiedVoiceSynthesis.speak({
    text,
    emotion,
    mode,
    engine
  })
}